{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DougChul/RNA/blob/Colab/relpos%2Bpairwise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5-trPEoGHlh",
        "outputId": "38197cd4-539f-480b-fcd2-f009dfd9251f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozyTvAptc2tY"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR5zVYDac2td"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#set seed for everything\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "random.seed(0)"
      ],
      "metadata": {
        "id": "JWMUZDKggOdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud1AE0c3c2te",
        "outputId": "46b1a981-d8a6-4ce9-eb5c-b8566b3d6de3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n",
            "2.2.2\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)\n",
        "print(pd.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Quozo9asc2th"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdapDbGTc2ti"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"seed\": 0,\n",
        "    \"cutoff_date\": \"2020-01-01\",\n",
        "    \"test_cutoff_date\": \"2022-05-01\",\n",
        "    \"max_len\": 384,  ##\n",
        "    \"batch_size\": 1,\n",
        "    \"learning_rate\": 0.3*1e-5,\n",
        "    \"weight_decay\": 0.0,\n",
        "    \"mixed_precision\": \"bf16\",\n",
        "    \"model_config_path\": \"../working/configs/pairwise.yaml\",  # Adjust path as needed\n",
        "    \"epochs\": 50,  ##\n",
        "    \"loss_power_scale\": 1.0,\n",
        "    \"max_cycles\": 1,\n",
        "    \"grad_clip\": 0.1,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"d_clamp\": 30,\n",
        "    \"max_len_filter\": 9999999,\n",
        "    \"min_len_filter\": 10,\n",
        "    \"structural_violation_epoch\": 50,\n",
        "    \"balance_weight\": False,\n",
        "    \"n_tokens\": 4,\n",
        "    \"d_model\": 256,  ##\n",
        "    \"n_heads\": 8,\n",
        "    \"dropout\": 0.1,\n",
        "    \"d_ff\": 1024, ##\n",
        "    \"norm_ratio\": 1.0, ##\n",
        "    \"n_layers\": 48, ##\n",
        "    \"pairwise_dimension\": 128,\n",
        "    \"dim_msa\": 32,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Tanb_d5c2tk"
      },
      "source": [
        "## Set Sample Data To make Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aC6yuPPcc2tl",
        "outputId": "0c7fcf82-d8e2-4f5d-ab31-12e55f1f60ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 217
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8H4DW11kc2tl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# folder_path = '/content/drive/MyDrive/RNA/stanford-rna-3d-folding'\n",
        "\n",
        "# ## Select Data\n",
        "# # set_data_fomat = '5models'\n",
        "# # set_data_fomat = 'v2'\n",
        "# set_data_fomat = 'v1'\n",
        "\n",
        "# if set_data_fomat == '5models':\n",
        "#     train_labels = pd.read_csv(os.path.join(folder_path, 'pdb_labels_5models.csv'))\n",
        "#     train_sequences = pd.read_csv(os.path.join(folder_path, 'pdb_sequences_5models.csv'))\n",
        "# elif set_data_fomat == 'v2':\n",
        "#     train_labels = pd.read_csv(os.path.join(folder_path, 'train_labels.v2.csv'))\n",
        "#     train_sequences = pd.read_csv(os.path.join(folder_path, 'train_sequences.v2.csv'))\n",
        "# elif set_data_fomat == 'v1':\n",
        "#     train_labels = pd.read_csv(os.path.join(folder_path, 'train_labels.csv'))\n",
        "#     train_sequences = pd.read_csv(os.path.join(folder_path, 'train_sequences.csv'))\n",
        "# else:\n",
        "#     raise ValueError(\"Invalid set_data_fomat\")\n",
        "\n",
        "\n",
        "# print(train_labels.head())\n",
        "\n",
        "# print(train_sequences.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tef5mNWDc2tm"
      },
      "outputs": [],
      "source": [
        "# train_labels[\"pdb_id\"] = train_labels[\"ID\"].apply(lambda x: x.split(\"_\")[0]+'_'+x.split(\"_\")[1])\n",
        "# # train_sequences[\"pdb_id\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpqzHHsqc2tm"
      },
      "outputs": [],
      "source": [
        "# all_xyz=[]\n",
        "\n",
        "# test_sample = False\n",
        "# count = 0\n",
        "\n",
        "# for pdb_id in tqdm(train_sequences['target_id']):\n",
        "#     df = train_labels[train_labels[\"pdb_id\"]==pdb_id]\n",
        "#     #break\n",
        "#     # xyz=df[['x_1','y_1','z_1','x_2','y_2','z_2','x_3','y_3','z_3','x_4','y_4','z_4','x_5','y_5','z_5',]].to_numpy().astype('float32')\n",
        "#     xyz=df[['x_1','y_1','z_1']].to_numpy().astype('float32')\n",
        "#     xyz[xyz<-1e17]=float('Nan');\n",
        "#     all_xyz.append(xyz)\n",
        "#     if test_sample == True:\n",
        "#       count += 1\n",
        "#       if count == 100:\n",
        "#         break\n",
        "# # all_xyz[13]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V2OfrEec2tm"
      },
      "source": [
        "### filter data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "am-8hZfmc2tn"
      },
      "outputs": [],
      "source": [
        "# # filter the data\n",
        "# # Filter and process data\n",
        "# filter_nan = []\n",
        "# max_len = 0\n",
        "# filter_ratio = 0 # All data are valid\n",
        "# for xyz in all_xyz:\n",
        "#     if len(xyz) > max_len:\n",
        "#         max_len = len(xyz)\n",
        "\n",
        "#     filter_nan.append((np.isnan(xyz).mean() <= filter_ratio) & \\\n",
        "#                       (len(xyz)<config['max_len_filter']) & \\\n",
        "#                       (len(xyz)>config['min_len_filter']))\n",
        "\n",
        "\n",
        "# print(f\"Longest sequence in train: {max_len}\")\n",
        "\n",
        "# filter_nan = np.array(filter_nan)\n",
        "# non_nan_indices = np.arange(len(filter_nan))[filter_nan]\n",
        "# print('remain sequences:', len(non_nan_indices))\n",
        "# train_sequences = train_sequences.loc[non_nan_indices].reset_index(drop=True)\n",
        "# non_nan_xyz=[all_xyz[i] for i in non_nan_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ooxt9tkzc2to"
      },
      "outputs": [],
      "source": [
        "# #pack data into a dictionary\n",
        "\n",
        "# data={\n",
        "#       \"pdb_id\":train_sequences['target_id'].to_list(),\n",
        "#       \"sequence\":train_sequences['sequence'].to_list(),\n",
        "#       \"temporal_cutoff\": train_sequences['temporal_cutoff'].to_list(),\n",
        "#       \"description\": train_sequences['description'].to_list(),\n",
        "#       \"all_sequences\": train_sequences['all_sequences'].to_list(),\n",
        "#       \"xyz\": non_nan_xyz\n",
        "# }\n",
        "# print(data['pdb_id'][2])\n",
        "# print(data['sequence'][1])\n",
        "# print(data['temporal_cutoff'][1])\n",
        "# # print(data['description'][1])\n",
        "# # print(data['all_sequences'][1])\n",
        "# # print(data['xyz'][1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('/content/drive/MyDrive/RNA/stanford-rna-3d-folding/processed_data.v1.pkl', 'wb') as f:\n",
        "#     pickle.dump(data, f)\n",
        "# print(\"데이터가 processed_data.pkl 파일로 저장되었습니다.\")"
      ],
      "metadata": {
        "id": "I3fj1aWcIBVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load Preprocess DAta\n",
        "\n",
        "with open('/content/drive/MyDrive/RNA/stanford-rna-3d-folding/processed_data.v2_max=384.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)"
      ],
      "metadata": {
        "id": "7LfEXiYJPGdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTOEWj85c2tp"
      },
      "source": [
        "### Split Train / Validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZYN3QxSc2tq"
      },
      "outputs": [],
      "source": [
        "# Split data into train and test\n",
        "all_index = np.arange(len(data['sequence']))\n",
        "cutoff_date = pd.Timestamp(config['cutoff_date'])\n",
        "test_cutoff_date = pd.Timestamp(config['test_cutoff_date'])\n",
        "train_index = [i for i, d in enumerate(data['temporal_cutoff']) if pd.Timestamp(d) <= cutoff_date]\n",
        "test_index = [i for i, d in enumerate(data['temporal_cutoff']) if pd.Timestamp(d) > cutoff_date and pd.Timestamp(d) <= test_cutoff_date]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rmby3eM5c2tq"
      },
      "source": [
        "### To Pytorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBbFjh1Xc2tq"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from ast import literal_eval\n",
        "\n",
        "def get_ct(bp,s):\n",
        "    ct_matrix=np.zeros((len(s),len(s)))\n",
        "    for b in bp:\n",
        "        ct_matrix[b[0]-1,b[1]-1]=1\n",
        "    return ct_matrix\n",
        "\n",
        "class RNA3D_Dataset(Dataset):\n",
        "    def __init__(self,indices,data):\n",
        "        self.indices=indices\n",
        "        self.data=data\n",
        "        self.tokens={nt:i for i,nt in enumerate('ACGU')}\n",
        "        # self.tokens = {'A': 2, 'U':-2, 'G':3, 'C':-3}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        idx=self.indices[idx]\n",
        "        sequence=[self.tokens[nt] for nt in (self.data['sequence'][idx])]\n",
        "        sequence=np.array(sequence)\n",
        "        sequence=torch.tensor(sequence)\n",
        "\n",
        "        #get C1' xyz\n",
        "        xyz=self.data['xyz'][idx]\n",
        "        xyz=torch.tensor(np.array(xyz))\n",
        "\n",
        "\n",
        "        if len(sequence)>config['max_len']:\n",
        "            crop_start=np.random.randint(len(sequence)-config['max_len'])\n",
        "            crop_end=crop_start+config['max_len']\n",
        "\n",
        "            sequence=sequence[crop_start:crop_end]\n",
        "            xyz=xyz[crop_start:crop_end]\n",
        "        #center at first atom if first atom does not exit go until it does\n",
        "        for i in range(len(xyz)):\n",
        "            if (~torch.isnan(xyz[i])).all():\n",
        "                break\n",
        "        xyz=xyz-xyz[i]\n",
        "\n",
        "        return {'pbd_id':self.data['pdb_id'][idx],\n",
        "            'sequence':sequence,\n",
        "                'xyz':xyz}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBsrRj10c2tr"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_dataset=RNA3D_Dataset(train_index,data)\n",
        "val_dataset=RNA3D_Dataset(test_index,data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Whap4bhc2ts"
      },
      "outputs": [],
      "source": [
        "train_loader=DataLoader(train_dataset,batch_size=1,shuffle=True)\n",
        "val_loader=DataLoader(val_dataset,batch_size=1,shuffle=False)\n",
        "\n",
        "# print(train_dataset[2]['xyz'][6])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyKuOInqc2ts"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y1zf5eAc2tt"
      },
      "source": [
        "### Example with one data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvdEweIFc2tt",
        "outputId": "c14e5ab5-2182-464b-82c8-8d48ecea5d9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n",
            "{'pbd_id': '3J0Q_3', 'sequence': tensor([2, 1, 1, 0, 2, 3, 2, 0, 0, 0, 3, 0]), 'xyz': tensor([[  0.0000,   0.0000,   0.0000],\n",
            "        [ -1.0760,  -3.3350,  -3.4670],\n",
            "        [ -4.4130,  -6.6660,  -7.6410],\n",
            "        [ -7.0260,  -0.6310,  -9.7380],\n",
            "        [ -3.1140,   3.6950, -11.0880],\n",
            "        [ -2.5960,   8.3430, -14.4300],\n",
            "        [  1.2760,  10.8900, -15.9760],\n",
            "        [  7.1280,  12.5880, -19.2280],\n",
            "        [  8.5000,   9.1200, -14.2150],\n",
            "        [  4.5950,   6.0570,  -8.3870],\n",
            "        [  6.2800,   5.2940,  -2.2990],\n",
            "        [  6.1480,   2.7090,   3.4880]])}\n",
            "1\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "test_sample = True\n",
        "\n",
        "if test_sample == True:\n",
        "  from torch.utils.data import Subset\n",
        "\n",
        "  target_index = 0\n",
        "  train_dataset = Subset(train_loader.dataset, [target_index])\n",
        "  train_loader = DataLoader(train_dataset, batch_size=1)\n",
        "\n",
        "  val_dataset = Subset(val_loader.dataset, [target_index])\n",
        "  val_loader = DataLoader(val_dataset, batch_size=1)\n",
        "\n",
        "print(len(train_dataset[0]['sequence']))\n",
        "print(train_dataset[0])\n",
        "print(len(train_loader))\n",
        "print(len(val_loader))\n",
        "# print(f\"새로운 데이터셋 크기: {len(single_data_loader.dataset)}\")\n",
        "# print(f\"새로운 DataLoader의 배치 개수: {len(single_data_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17wCulJ0c2tu"
      },
      "source": [
        "### Check Pairwise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgYG2Xm-c2tu"
      },
      "outputs": [],
      "source": [
        "# import RNA\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# src = torch.tensor([2, 2, 1, 2, 3, 0, 0, 2, 2, 0, 3, 3, 0, 1, 1, 3, 0, 3, 2, 1, 1], dtype=torch.long)\n",
        "# # AUGC를 0123으로 매핑\n",
        "# nucleotide_map = {0: 'A', 1: 'C', 2: 'G', 3: 'U'}\n",
        "\n",
        "# str_A = ''.join([nucleotide_map[x.item()] for x in src])\n",
        "\n",
        "# print(len(str_A))\n",
        "# print(str_A)\n",
        "\n",
        "# md = RNA.md()\n",
        "\n",
        "# fc = RNA.fold_compound(str_A, md)\n",
        "\n",
        "# # predict Minmum Free Energy and corresponding secondary structure\n",
        "# # (ss, mfe) =\n",
        "# # print(fc.mfe())\n",
        "# fc.pf()\n",
        "# B = torch.tensor(fc.bpp())\n",
        "\n",
        "# # B = torch.tensor(B)\n",
        "# B = B[1:,1:]\n",
        "# print(type(B))\n",
        "# print(B.shape)\n",
        "# B = B + B.T\n",
        "# # B = np.array(B, dtype=np.bool())\n",
        "# # print(B)\n",
        "\n",
        "# plt.figure(figsize=(5,4))\n",
        "# sns.heatmap(B, cmap='viridis', annot=False)  # annot=True로 설정하면 각 셀에 값 표시\n",
        "# plt.title('Pairwise Contact Probability Matrix')\n",
        "# plt.show()\n",
        "\n",
        "# # def get_pairwise_features(src, seq_len, d_model):\n",
        "# #     nucleotide_map = {0: 'A', 1: 'C', 2: 'G', 3: 'U'}\n",
        "\n",
        "# #     # print(\"pairwise\",src)\n",
        "# #     src = src.squeeze(0)\n",
        "# #     str_seq = ''.join([nucleotide_map[x.item()] for x in src])\n",
        "# #     # print(\"str_seq\",str_seq)\n",
        "\n",
        "# #     md = RNA.md()\n",
        "# #     fc = RNA.fold_compound(str_seq, md)\n",
        "# #     fc.pf() ##???\n",
        "\n",
        "# #     pair_matrix = torch.tensor(fc.bpp(), dtype=torch.float32)\n",
        "# #     # print(\"pair_matrix\",pair_matrix)\n",
        "# #     pair_matrix = pair_matrix[1:,1:] # remove first row and column 0 index in bpp is always 0\n",
        "# #     pair_matrix = pair_matrix + pair_matrix.T # symmetric matrix\n",
        "\n",
        "# #     pair_matrix = pair_matrix.unsqueeze(0).unsqueeze(0)\n",
        "# #     pair_matrix = F.interpolate(pair_matrix, size=(d_model, d_model), mode='bilinear', align_corners=False)\n",
        "# #     # pair_matrix = F.interpolate(pair_matrix, size=(d_model, d_model), mode='nearest', align_corners=False)\n",
        "\n",
        "# #     pair_matrix = pair_matrix.squeeze()\n",
        "# #     # print(\"pair_matrix\",pair_matrix.shape)\n",
        "\n",
        "# #     return pair_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8ueTbF_c2tv"
      },
      "source": [
        "### Pairwise Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUYNiz6jc2tw"
      },
      "outputs": [],
      "source": [
        "from torch import einsum\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "class OuterProductMean(nn.Module):\n",
        "    def __init__(self, in_dim=256, dim_msa=32, pairwise_dim=128):\n",
        "        super(OuterProductMean, self).__init__()\n",
        "        self.proj_down1 = nn.Linear(in_dim, dim_msa)\n",
        "        self.proj_down2 = nn.Linear(dim_msa ** 2, pairwise_dim)\n",
        "\n",
        "    def forward(self,seq_rep, pair_rep=None):\n",
        "        seq_rep=self.proj_down1(seq_rep)\n",
        "        outer_product = torch.einsum('bid,bjc -> bijcd', seq_rep, seq_rep)\n",
        "        outer_product = rearrange(outer_product, 'b i j c d -> b i j (c d)')\n",
        "        outer_product = self.proj_down2(outer_product)\n",
        "\n",
        "        if pair_rep is not None:\n",
        "            outer_product=outer_product+pair_rep\n",
        "\n",
        "        return outer_product\n",
        "\n",
        "\n",
        "class TriangleAttention(nn.Module):\n",
        "    def __init__(self, in_dim=128, dim=32, n_heads=4, wise='row'):\n",
        "        super(TriangleAttention, self).__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.wise = wise\n",
        "        self.norm = nn.LayerNorm(in_dim)\n",
        "        self.to_qkv = nn.Linear(in_dim, dim * 3 * n_heads, bias=False)\n",
        "        self.linear_for_pair = nn.Linear(in_dim, n_heads, bias=False)\n",
        "        self.to_gate = nn.Sequential(\n",
        "            nn.Linear(in_dim, in_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.to_out = nn.Linear(n_heads * dim, in_dim)\n",
        "        # self.to_out.weight.data.fill_(0.)\n",
        "        # self.to_out.bias.data.fill_(0.)\n",
        "\n",
        "    def forward(self, z, src_mask):\n",
        "        \"\"\"\n",
        "        how to do masking\n",
        "        for row tri attention:\n",
        "        attention matrix is brijh, where b is batch, r is row, h is head\n",
        "        so mask should be b()ijh, i.e. take self attention mask and unsqueeze(1,-1)\n",
        "        add negative inf to matrix before softmax\n",
        "\n",
        "        for col tri attention\n",
        "        attention matrix is bijlh, so take self attention mask and unsqueeze(3,-1)\n",
        "\n",
        "        take src_mask and spawn pairwise mask, and unsqueeze accordingly\n",
        "        \"\"\"\n",
        "\n",
        "        #spwan pair mask\n",
        "        # print('spwan pair mask')\n",
        "        src_mask[src_mask==0]=-1\n",
        "        src_mask=src_mask.unsqueeze(-1).float()\n",
        "\n",
        "        attn_mask=torch.matmul(src_mask,src_mask.permute(0,2,1))\n",
        "\n",
        "\n",
        "        wise = self.wise\n",
        "        z = self.norm(z)\n",
        "        q, k, v = torch.chunk(self.to_qkv(z), 3, -1)\n",
        "        q, k, v = map(lambda x: rearrange(x, 'b i j (h d)->b i j h d', h=self.n_heads), (q, k, v))\n",
        "        b = self.linear_for_pair(z)\n",
        "        gate = self.to_gate(z)\n",
        "        scale = q.size(-1) ** .5\n",
        "        if wise == 'row':\n",
        "            eq_attn = 'brihd,brjhd->brijh'\n",
        "            eq_multi = 'brijh,brjhd->brihd'\n",
        "            b = rearrange(b, 'b i j (r h)->b r i j h', r=1)\n",
        "            softmax_dim = 3\n",
        "            attn_mask=rearrange(attn_mask, 'b i j->b 1 i j 1')\n",
        "        elif wise == 'col':\n",
        "            eq_attn = 'bilhd,bjlhd->bijlh'\n",
        "            eq_multi = 'bijlh,bjlhd->bilhd'\n",
        "            b = rearrange(b, 'b i j (l h)->b i j l h', l=1)\n",
        "            softmax_dim = 2\n",
        "            attn_mask=rearrange(attn_mask, 'b i j->b i j 1 1')\n",
        "        else:\n",
        "            raise ValueError('wise should be col or row!')\n",
        "        logits = (torch.einsum(eq_attn, q, k) / scale + b)\n",
        "        # plt.imshow(attn_mask[0,0,:,:,0])\n",
        "        # plt.show()\n",
        "        # exit()\n",
        "        logits = logits.masked_fill(attn_mask == -1, float('-1e-9'))\n",
        "        attn = logits.softmax(softmax_dim)\n",
        "        # print(attn.shape)\n",
        "        # print(v.shape)\n",
        "        out = torch.einsum(eq_multi, attn, v)\n",
        "        out = gate * rearrange(out, 'b i j h d-> b i j (h d)')\n",
        "        z_ = self.to_out(out)\n",
        "        return z_\n",
        "\n",
        "\n",
        "class TriangleMultiplicativeModule(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        hidden_dim = None,\n",
        "        mix = 'ingoing'\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert mix in {'ingoing', 'outgoing'}, 'mix must be either ingoing or outgoing'\n",
        "\n",
        "        hidden_dim = default(hidden_dim, dim)\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "        self.left_proj = nn.Linear(dim, hidden_dim)\n",
        "        self.right_proj = nn.Linear(dim, hidden_dim)\n",
        "\n",
        "        self.left_gate = nn.Linear(dim, hidden_dim)\n",
        "        self.right_gate = nn.Linear(dim, hidden_dim)\n",
        "        self.out_gate = nn.Linear(dim, hidden_dim)\n",
        "\n",
        "        # initialize all gating to be identity\n",
        "\n",
        "        for gate in (self.left_gate, self.right_gate, self.out_gate):\n",
        "            nn.init.constant_(gate.weight, 0.)\n",
        "            nn.init.constant_(gate.bias, 1.)\n",
        "\n",
        "        if mix == 'outgoing':\n",
        "            self.mix_einsum_eq = '... i k d, ... j k d -> ... i j d'\n",
        "        elif mix == 'ingoing':\n",
        "            self.mix_einsum_eq = '... k j d, ... k i d -> ... i j d'\n",
        "\n",
        "        self.to_out_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.to_out = nn.Linear(hidden_dim, dim)\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        # print(src_mask.shape)\n",
        "        src_mask=src_mask.unsqueeze(-1).float()\n",
        "        mask = torch.matmul(src_mask,src_mask.permute(0,2,1))\n",
        "        assert x.shape[1] == x.shape[2], 'feature map must be symmetrical'\n",
        "        if exists(mask):\n",
        "            mask = rearrange(mask, 'b i j -> b i j ()')\n",
        "\n",
        "        x = self.norm(x)\n",
        "\n",
        "        left = self.left_proj(x)\n",
        "        right = self.right_proj(x)\n",
        "\n",
        "        if exists(mask):\n",
        "            left = left * mask\n",
        "            right = right * mask\n",
        "\n",
        "        left_gate = self.left_gate(x).sigmoid()\n",
        "        right_gate = self.right_gate(x).sigmoid()\n",
        "        out_gate = self.out_gate(x).sigmoid()\n",
        "\n",
        "        left = left * left_gate\n",
        "        right = right * right_gate\n",
        "\n",
        "        out = einsum(self.mix_einsum_eq, left, right)\n",
        "\n",
        "        out = self.to_out_norm(out)\n",
        "        out = out * out_gate\n",
        "        return self.to_out(out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dropout Module"
      ],
      "metadata": {
        "id": "uTtot2QwoCHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partialmethod\n",
        "from typing import Union, List\n",
        "\n",
        "\n",
        "class Dropout(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of dropout with the ability to share the dropout mask\n",
        "    along a particular dimension.\n",
        "\n",
        "    If not in training mode, this module computes the identity function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, r: float, batch_dim: Union[int, List[int]]):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            r:\n",
        "                Dropout rate\n",
        "            batch_dim:\n",
        "                Dimension(s) along which the dropout mask is shared\n",
        "        \"\"\"\n",
        "        super(Dropout, self).__init__()\n",
        "\n",
        "        self.r = r\n",
        "        if type(batch_dim) == int:\n",
        "            batch_dim = [batch_dim]\n",
        "        self.batch_dim = batch_dim\n",
        "        self.dropout = nn.Dropout(self.r)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x:\n",
        "                Tensor to which dropout is applied. Can have any shape\n",
        "                compatible with self.batch_dim\n",
        "        \"\"\"\n",
        "        shape = list(x.shape)\n",
        "        if self.batch_dim is not None:\n",
        "            for bd in self.batch_dim:\n",
        "                shape[bd] = 1\n",
        "        mask = x.new_ones(shape)\n",
        "        mask = self.dropout(mask)\n",
        "        x = x * mask\n",
        "        return x\n",
        "\n",
        "\n",
        "class DropoutRowwise(Dropout):\n",
        "    \"\"\"\n",
        "    Convenience class for rowwise dropout as described in subsection\n",
        "    1.11.6.\n",
        "    \"\"\"\n",
        "\n",
        "    __init__ = partialmethod(Dropout.__init__, batch_dim=-3)\n",
        "\n",
        "\n",
        "class DropoutColumnwise(Dropout):\n",
        "    \"\"\"\n",
        "    Convenience class for columnwise dropout as described in subsection\n",
        "    1.11.6.\n",
        "    \"\"\"\n",
        "\n",
        "    __init__ = partialmethod(Dropout.__init__, batch_dim=-2)"
      ],
      "metadata": {
        "id": "6U3fJJGSoFUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mrsxE-qc2tx"
      },
      "source": [
        "### Back Bone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHBhYHxEc2tx"
      },
      "outputs": [],
      "source": [
        "# import RNA\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat, reduce\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout, max_len=config['max_len']):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Compute the positional encodings once in log space.\n",
        "        self.pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
        "        )\n",
        "        self.pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.pe = self.pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", self.pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class relpos(nn.Module):\n",
        "\n",
        "    def __init__(self, dim=64):\n",
        "        super(relpos, self).__init__()\n",
        "        self.linear = nn.Linear(33, dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        L=src.shape[1]\n",
        "        res_id = torch.arange(L).to(src.device).unsqueeze(0)\n",
        "        device = res_id.device\n",
        "        bin_values = torch.arange(-16, 17, device=device)\n",
        "        #print((bin_values))\n",
        "        d = res_id[:, :, None] - res_id[:, None, :]\n",
        "        bdy = torch.tensor(16, device=device)\n",
        "        d = torch.minimum(torch.maximum(-bdy, d), bdy)\n",
        "        d_onehot = (d[..., None] == bin_values).float()\n",
        "        #print(d_onehot.sum(dim=-1).min())\n",
        "        assert d_onehot.sum(dim=-1).min() == 1\n",
        "        p = self.linear(d_onehot)\n",
        "        return p\n",
        "\n",
        "# class ConstrainedPositionalEncoding(nn.Module):\n",
        "#     def __init__(self, d_model, max_relative_position=32, constrained_position=4):\n",
        "#         super().__init__()\n",
        "#         self.max_relative_position = max_relative_position\n",
        "#         self.relative_embedding = nn.Embedding(2 * max_relative_position + 1, d_model)\n",
        "#         self.constrained_position = constrained_position\n",
        "\n",
        "#     def forward(self, seq_len):\n",
        "\n",
        "#         return pos_encoding(seq_len, self.d_model)\n",
        "\n",
        "\n",
        "class EmbedSequence(nn.Module):\n",
        "    def __init__(self, d_model, out_dim, n_tokens = config['n_tokens']):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedder = nn.Embedding(n_tokens, d_model)\n",
        "        self.outer_product_mean = OuterProductMean(pairwise_dim=out_dim)\n",
        "        self.pos_encoder = relpos(out_dim)\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        B,L = sequence.shape\n",
        "        # print(seq_len)\n",
        "        # pos_encoding = pos_encoder(seq_len, self.d_model, sequence.device)\n",
        "        sequence = sequence.long()\n",
        "        sequence = self.embedder(sequence).reshape(B,L,-1)\n",
        "\n",
        "        pairwise_feature = self.outer_product_mean(sequence)\n",
        "        pairwise_feature = pairwise_feature + self.pos_encoder(sequence)\n",
        "\n",
        "        return sequence, pairwise_feature\n",
        "\n",
        "\n",
        "class MultiheadAtt(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=8, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_model // n_heads\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.w_q = nn.Linear(d_model, n_heads * self.d_head)\n",
        "        self.w_k = nn.Linear(d_model, n_heads * self.d_head)\n",
        "        self.w_v = nn.Linear(d_model, n_heads * self.d_head)\n",
        "\n",
        "        self.w_o = nn.Linear(n_heads * self.d_head, d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # print('query.shape',query.shape)\n",
        "        batch_size, length, d_model = query.size()\n",
        "\n",
        "        q = self.w_q(query)\n",
        "        k = self.w_k(key)\n",
        "        v = self.w_v(value)\n",
        "\n",
        "        q = q.view(batch_size, length, self.n_heads, self.d_head)\n",
        "        k = k.view(batch_size, length, self.n_heads, self.d_head)\n",
        "        v = v.view(batch_size, length, self.n_heads, self.d_head)\n",
        "\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
        "        if mask is not None: # pairwise bias\n",
        "          scores = scores + mask  # For head axis broadcasting\n",
        "        scores = self.dropout_1(F.softmax(scores, dim=-1))\n",
        "\n",
        "        output = torch.matmul(scores, v)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, length, -1)\n",
        "\n",
        "        output = self.dropout_2(self.w_o(output))\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.w_1(x)\n",
        "        x = self.activation(x)\n",
        "        if self.dropout is not None:\n",
        "            x = self.dropout(x)\n",
        "        x = self.w_2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, config=config, dropout=0, norm_type='post_ln'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_ff = config['d_ff']\n",
        "\n",
        "        pairwise_dimension=config['pairwise_dimension']\n",
        "        self.outer_product_mean = OuterProductMean(pairwise_dim=pairwise_dimension)\n",
        "\n",
        "        # self.use_triangular_attention=config['use_triangular_attention']\n",
        "\n",
        "        if (norm_type == 'post_ln') or (norm_type == 'pre_ln'):\n",
        "            self.norm_type = norm_type\n",
        "        else:\n",
        "            raise ValueError(\"Invalid norm_type\")\n",
        "\n",
        "        self.norm_0 = nn.LayerNorm(d_model)\n",
        "        self.norm_1 = nn.LayerNorm(d_model)\n",
        "        self.norm_2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.attention = MultiheadAtt(d_model, n_heads, dropout)\n",
        "        self.feedforward = FeedForward(d_model, self.d_ff, dropout)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "        self.pairwise_norm = nn.LayerNorm(pairwise_dimension)\n",
        "        self.pairwise2heads=nn.Linear(pairwise_dimension,n_heads,bias=False) #nhead = 8\n",
        "\n",
        "        ## triangle\n",
        "        self.triangle_update_out=TriangleMultiplicativeModule(dim=pairwise_dimension,mix='outgoing')\n",
        "        self.triangle_update_in=TriangleMultiplicativeModule(dim=pairwise_dimension,mix='ingoing')\n",
        "\n",
        "        self.pair_dropout_out=DropoutRowwise(dropout) # dropout.py batch_dim=-3 q=0.1 row dropout\n",
        "        self.pair_dropout_in=DropoutRowwise(dropout)  # dropout.py batch_dim=-2 q=0.1 column dropout\n",
        "\n",
        "        self.triangle_attention_out=TriangleAttention(in_dim=pairwise_dimension,\n",
        "                                                                    dim=pairwise_dimension//4,\n",
        "                                                                    wise='row')\n",
        "        self.triangle_attention_in=TriangleAttention(in_dim=pairwise_dimension,\n",
        "                                                                    dim=pairwise_dimension//4,\n",
        "                                                                    wise='col')\n",
        "\n",
        "        self.pair_attention_dropout_out=DropoutRowwise(dropout)\n",
        "        self.pair_attention_dropout_in=DropoutColumnwise(dropout)\n",
        "\n",
        "        self.pair_transition=nn.Sequential(\n",
        "                                           nn.LayerNorm(pairwise_dimension),\n",
        "                                           nn.Linear(pairwise_dimension,pairwise_dimension*4),\n",
        "                                           nn.ReLU(inplace=True),\n",
        "                                           nn.Linear(pairwise_dimension*4,pairwise_dimension))\n",
        "\n",
        "\n",
        "    def forward(self, src, pairwise_features=None, src_mask=None, use_gradient_checkpoint=False):\n",
        "        # src = residual_connection(src, self.encoder(src))\n",
        "        norm_type = self.norm_type\n",
        "        pairwise_bias = self.pairwise2heads(self.pairwise_norm(pairwise_features)).permute(0,3,1,2)\n",
        "\n",
        "        if norm_type == 'post_ln':\n",
        "            src = self.norm_0(src)\n",
        "            src = src + self.attention(src,src,src,mask=pairwise_bias)  #residual conn\n",
        "            src = self.norm_1(src)\n",
        "            # src = self.activation(src)\n",
        "            src = src + self.feedforward(src)  #residual conn\n",
        "            src = self.norm_2(src)\n",
        "            # src = self.activation(src)\n",
        "\n",
        "        elif norm_type == 'pre_ln':\n",
        "            src_temp = src\n",
        "            src = self.norm_0(src)\n",
        "            src = self.attention(src,src,src) + src_temp\n",
        "            # src = self.activation(src)\n",
        "            src_temp = src\n",
        "            src = self.norm_1(src)\n",
        "            src = self.feedforward(src) + src_temp\n",
        "            # src = self.activation(src)\n",
        "            src = self.norm_2(src)\n",
        "\n",
        "        if use_gradient_checkpoint:\n",
        "            pairwise_features=pairwise_features+checkpoint.checkpoint(self.custom(self.outer_product_mean), src)\n",
        "            pairwise_features=pairwise_features+self.pair_dropout_out(\n",
        "                checkpoint.checkpoint(self.custom(self.triangle_update_out), pairwise_features, src_mask))\n",
        "            pairwise_features=pairwise_features+self.pair_dropout_in(\n",
        "                checkpoint.checkpoint(self.custom(self.triangle_update_in), pairwise_features, src_mask))\n",
        "\n",
        "        else:\n",
        "            pairwise_features=pairwise_features+self.outer_product_mean(src)\n",
        "            pairwise_features=pairwise_features+self.pair_dropout_out(self.triangle_update_out(pairwise_features,src_mask))\n",
        "            pairwise_features=pairwise_features+self.pair_dropout_in(self.triangle_update_in(pairwise_features,src_mask))\n",
        "\n",
        "        # if self.use_triangular_attention:\n",
        "        pairwise_features=pairwise_features+self.pair_attention_dropout_out(self.triangle_attention_out(pairwise_features,src_mask))\n",
        "        pairwise_features=pairwise_features+self.pair_attention_dropout_in(self.triangle_attention_in(pairwise_features,src_mask))\n",
        "\n",
        "        if use_gradient_checkpoint:\n",
        "            pairwise_features=pairwise_features+checkpoint.checkpoint(self.custom(self.pair_transition),pairwise_features)\n",
        "        else:\n",
        "            pairwise_features=pairwise_features+self.pair_transition(pairwise_features)\n",
        "\n",
        "        return src, pairwise_features\n",
        "\n",
        "\n",
        "class WuSubSol(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(WuSubSol, self).__init__()\n",
        "        self.config = config\n",
        "        self.n_layers = config['n_layers']\n",
        "        self.d_model = config['d_model']\n",
        "\n",
        "        self.norm_ratio = config['norm_ratio']\n",
        "        if (self.n_layers * self.norm_ratio).is_integer():\n",
        "            self.n_post_ln = int(self.n_layers * self.norm_ratio)\n",
        "        else:\n",
        "            print(\"Invalid norm_ratio\")\n",
        "            exit()\n",
        "\n",
        "        self.embedding = EmbedSequence(self.d_model, config['pairwise_dimension'])\n",
        "\n",
        "        self.encoder_layers = []\n",
        "        count_post_ln, count_pre_ln = 0,0\n",
        "        for i in range(self.n_post_ln):\n",
        "            self.encoder_layers.append(EncoderLayer(self.d_model, config['n_heads'], config, config['dropout'], 'post_ln'))\n",
        "            count_post_ln += 1\n",
        "        for i in range(self.n_layers - self.n_post_ln):\n",
        "            self.encoder_layers.append(EncoderLayer(self.d_model, config['n_heads'], config['dropout'], 'pre_ln'))\n",
        "            count_pre_ln += 1\n",
        "\n",
        "        print(f\"{count_post_ln} post_ln layers and {count_pre_ln} pre_ln layers\")\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList(self.encoder_layers)\n",
        "\n",
        "        self.final_linear = nn.Linear(self.d_model, 3)\n",
        "\n",
        "        print(f\"{self.n_layers} layers of encoder constructed\")\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "\n",
        "        src_mask = torch.ones_like(src).to(src.device)\n",
        "        src, pairwise_feature = self.embedding(src) # L*d_model\n",
        "        # print('after embedding', pairwise_feature.shape, pairwise_feature)\n",
        "\n",
        "\n",
        "        for i,layer in enumerate(self.encoder_layers):\n",
        "            # print(\"before\",src.shape)\n",
        "            src, pairwise_feature = layer(src, pairwise_feature, src_mask)\n",
        "            # print(\"after\",src.shape)\n",
        "            # print(src, pairwise_feature)\n",
        "\n",
        "        src = self.final_linear(src).squeeze()\n",
        "        for i in range(len(src)):\n",
        "            if (~torch.isnan(src[i])).all():\n",
        "                break\n",
        "        src=src-src[i]\n",
        "\n",
        "        # print('final tensor-Shape : ', src.shape)\n",
        "\n",
        "        return src\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV-uV_vHc2tz",
        "outputId": "dd5b56c7-b910-4203-8caa-e46d42b8ce6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48 post_ln layers and 0 pre_ln layers\n",
            "48 layers of encoder constructed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WuSubSol(\n",
              "  (embedding): EmbedSequence(\n",
              "    (embedder): Embedding(4, 256)\n",
              "    (outer_product_mean): OuterProductMean(\n",
              "      (proj_down1): Linear(in_features=256, out_features=32, bias=True)\n",
              "      (proj_down2): Linear(in_features=1024, out_features=128, bias=True)\n",
              "    )\n",
              "    (pos_encoder): relpos(\n",
              "      (linear): Linear(in_features=33, out_features=128, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (encoder_layers): ModuleList(\n",
              "    (0-47): 48 x EncoderLayer(\n",
              "      (outer_product_mean): OuterProductMean(\n",
              "        (proj_down1): Linear(in_features=256, out_features=32, bias=True)\n",
              "        (proj_down2): Linear(in_features=1024, out_features=128, bias=True)\n",
              "      )\n",
              "      (norm_0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (attention): MultiheadAtt(\n",
              "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
              "        (w_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (w_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (w_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (w_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (w_1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "        (w_2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "        (activation): ReLU()\n",
              "      )\n",
              "      (activation): ReLU()\n",
              "      (pairwise_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (pairwise2heads): Linear(in_features=128, out_features=8, bias=False)\n",
              "      (triangle_update_out): TriangleMultiplicativeModule(\n",
              "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (left_proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (right_proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (left_gate): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (right_gate): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (out_gate): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (to_out_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (triangle_update_in): TriangleMultiplicativeModule(\n",
              "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (left_proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (right_proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (left_gate): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (right_gate): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (out_gate): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (to_out_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (pair_dropout_out): DropoutRowwise(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (pair_dropout_in): DropoutRowwise(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (triangle_attention_out): TriangleAttention(\n",
              "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (to_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
              "        (linear_for_pair): Linear(in_features=128, out_features=4, bias=False)\n",
              "        (to_gate): Sequential(\n",
              "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (1): Sigmoid()\n",
              "        )\n",
              "        (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (triangle_attention_in): TriangleAttention(\n",
              "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (to_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
              "        (linear_for_pair): Linear(in_features=128, out_features=4, bias=False)\n",
              "        (to_gate): Sequential(\n",
              "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (1): Sigmoid()\n",
              "        )\n",
              "        (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (pair_attention_dropout_out): DropoutRowwise(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (pair_attention_dropout_in): DropoutColumnwise(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (pair_transition): Sequential(\n",
              "        (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (1): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (final_linear): Linear(in_features=256, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 234
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = WuSubSol(config)\n",
        "model.to(device)\n",
        "# print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_IUreJxc2t2"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJLQJrhEc2t5"
      },
      "source": [
        "### Define Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hKk0tdlc2t5"
      },
      "outputs": [],
      "source": [
        "def calculate_distance_matrix(X,Y,epsilon=1e-4):\n",
        "    return (torch.square(X[:,None]-Y[None,:])+epsilon).sum(-1).sqrt()\n",
        "\n",
        "def dRMAE(pred_x,\n",
        "          pred_y,\n",
        "          gt_x,\n",
        "          gt_y,\n",
        "          epsilon=1e-4,Z=10,d_clamp=None):\n",
        "    pred_dm=calculate_distance_matrix(pred_x,pred_y)\n",
        "    gt_dm=calculate_distance_matrix(gt_x,gt_y)\n",
        "\n",
        "    mask=~torch.isnan(gt_dm)\n",
        "    mask[torch.eye(mask.shape[0]).bool()]=False\n",
        "\n",
        "    rmsd=torch.abs(pred_dm[mask]-gt_dm[mask])\n",
        "\n",
        "    return rmsd.mean()/Z\n",
        "\n",
        "def align_svd_mae(input, target, Z=10):\n",
        "    \"\"\"\n",
        "    Aligns the input (Nx3) to target (Nx3) using SVD-based Procrustes alignment\n",
        "    and computes RMSD loss.\n",
        "\n",
        "    Args:\n",
        "        input (torch.Tensor): Nx3 tensor representing the input points.\n",
        "        target (torch.Tensor): Nx3 tensor representing the target points.\n",
        "\n",
        "    Returns:\n",
        "        aligned_input (torch.Tensor): Nx3 aligned input.\n",
        "        rmsd_loss (torch.Tensor): RMSD loss.\n",
        "    \"\"\"\n",
        "    # print('input-Shape', input.shape)\n",
        "    # print('output-Shape', target.shape)\n",
        "    # target = target[:, :3]\n",
        "    assert input.shape == target.shape, \"Input and target must have the same shape\"\n",
        "\n",
        "    #mask\n",
        "    mask=~torch.isnan(target.sum(-1))\n",
        "\n",
        "    input=input[mask]\n",
        "    target=target[mask]\n",
        "\n",
        "    # Compute centroids\n",
        "    centroid_input = input.mean(dim=0, keepdim=True)\n",
        "    centroid_target = target.mean(dim=0, keepdim=True)\n",
        "\n",
        "    # Center the points\n",
        "    input_centered = input - centroid_input.detach()\n",
        "    target_centered = target - centroid_target\n",
        "\n",
        "    # Compute covariance matrix\n",
        "    cov_matrix = input_centered.T @ target_centered\n",
        "\n",
        "    # SVD to find optimal rotation\n",
        "    U, S, Vt = torch.svd(cov_matrix)\n",
        "\n",
        "    # Compute rotation matrix\n",
        "    R = Vt @ U.T\n",
        "\n",
        "    # Ensure a proper rotation (det(R) = 1, no reflection)\n",
        "    if torch.det(R) < 0:\n",
        "        Vt[-1, :] *= -1\n",
        "        R = Vt @ U.T\n",
        "\n",
        "    # Rotate input\n",
        "    aligned_input = (input_centered @ R.T.detach()) + centroid_target.detach()\n",
        "\n",
        "    # # Compute RMSD loss\n",
        "    # rmsd_loss = torch.sqrt(((aligned_input - target) ** 2).mean())\n",
        "\n",
        "    # rmsd_loss = torch.sqrt(((aligned_input - target) ** 2).mean())\n",
        "\n",
        "    # return aligned_input, rmsd_loss\n",
        "    return torch.abs(aligned_input-target).mean()/Z\n",
        "\n",
        "def align_svd_rmsd(input, target):\n",
        "    \"\"\"\n",
        "    Aligns the input (Nx3) to target (Nx3) using SVD-based Procrustes alignment\n",
        "    and computes RMSD loss.\n",
        "\n",
        "    Args:\n",
        "        input (torch.Tensor): Nx3 tensor representing the input points.\n",
        "        target (torch.Tensor): Nx3 tensor representing the target points.\n",
        "\n",
        "    Returns:\n",
        "        aligned_input (torch.Tensor): Nx3 aligned input.\n",
        "        rmsd_loss (torch.Tensor): RMSD loss.\n",
        "    \"\"\"\n",
        "    # print('input-Shape', input.shape)\n",
        "    # print('output-Shape', target.shape)\n",
        "    # target = target[:,:3]\n",
        "    assert input.shape == target.shape, \"Input and target must have the same shape\"\n",
        "\n",
        "    #mask\n",
        "    mask=~torch.isnan(target.sum(-1))\n",
        "\n",
        "\n",
        "    input=input[mask]\n",
        "    target=target[mask]\n",
        "\n",
        "    # Compute centroids\n",
        "    centroid_input = input.mean(dim=0, keepdim=True)\n",
        "    centroid_target = target.mean(dim=0, keepdim=True)\n",
        "\n",
        "    # Center the points\n",
        "    input_centered = input - centroid_input.detach()\n",
        "    target_centered = target - centroid_target\n",
        "\n",
        "    # Compute covariance matrix\n",
        "    cov_matrix = input_centered.T @ target_centered\n",
        "\n",
        "    # SVD to find optimal rotation\n",
        "    U, S, Vt = torch.svd(cov_matrix)\n",
        "\n",
        "    # Compute rotation matrix\n",
        "    R = Vt @ U.T\n",
        "\n",
        "    # Ensure a proper rotation (det(R) = 1, no reflection)\n",
        "    if torch.det(R) < 0:\n",
        "        Vt[-1, :] *= -1\n",
        "        R = Vt @ U.T\n",
        "\n",
        "    # Rotate input\n",
        "    aligned_input = (input_centered @ R.T.detach()) + centroid_target.detach()\n",
        "\n",
        "    # # Compute RMSD loss\n",
        "    # rmsd_loss = torch.sqrt(((aligned_input - target) ** 2).mean())\n",
        "\n",
        "    # rmsd_loss = torch.sqrt(((aligned_input - target) ** 2).mean())\n",
        "\n",
        "    # return aligned_input, rmsd_loss\n",
        "    return torch.square(aligned_input-target).mean().sqrt()\n",
        "\n",
        "def compute_lddt(ground_truth_atoms, predicted_atoms, cutoff=30.0, thresholds=[1.0, 2.0, 4.0, 8.0]):\n",
        "    \"\"\"\n",
        "    Computes the lDDT score between ground truth and predicted atoms.\n",
        "\n",
        "    Parameters:\n",
        "        ground_truth_atoms (np.array): Nx3 array of ground truth atom coordinates.\n",
        "        predicted_atoms (np.array): Nx3 array of predicted atom coordinates.\n",
        "        cutoff (float): Distance cutoff in Ångstroms to consider neighbors. Default is 30 Å.\n",
        "        thresholds (list): List of thresholds in Ångstroms for the lDDT computation. Default is [0.5, 1.0, 2.0, 4.0].\n",
        "\n",
        "    Returns:\n",
        "        float: The lDDT score.\n",
        "    \"\"\"\n",
        "    # Number of atoms\n",
        "    num_atoms = ground_truth_atoms.shape[0]\n",
        "\n",
        "    # Initialize array to store lDDT fractions for each threshold\n",
        "    fractions = np.zeros(len(thresholds))\n",
        "\n",
        "    for i in range(num_atoms):\n",
        "        # Get the distances from atom i to all other atoms for both ground truth and predicted atoms\n",
        "        gt_distances = np.linalg.norm(ground_truth_atoms[i] - ground_truth_atoms, axis=1)\n",
        "        pred_distances = np.linalg.norm(predicted_atoms[i] - predicted_atoms, axis=1)\n",
        "\n",
        "        # print(gt_distances)\n",
        "        # print(pred_distances)\n",
        "        # exit()\n",
        "        # Apply the cutoff to consider only distances within the cutoff range\n",
        "        mask = (gt_distances > 0) & (gt_distances < cutoff)\n",
        "\n",
        "        # Calculate the absolute difference between ground truth and predicted distances\n",
        "        distance_diff = np.abs(gt_distances[mask] - pred_distances[mask])\n",
        "\n",
        "        # Filter out any NaN values from the distance difference calculation\n",
        "        valid_mask = ~np.isnan(distance_diff)\n",
        "        distance_diff = distance_diff[valid_mask]\n",
        "\n",
        "        # Compute the fractions for each threshold\n",
        "        for j, threshold in enumerate(thresholds):\n",
        "            if len(distance_diff)>0:\n",
        "                fractions[j] += np.mean(distance_diff < threshold)\n",
        "    # print(fractions)\n",
        "    # print(num_atoms)\n",
        "\n",
        "    # Average the fractions over the number of atoms\n",
        "    fractions /= num_atoms\n",
        "\n",
        "    # The final lDDT score is the average of these fractions\n",
        "    lddt_score = np.mean(fractions)\n",
        "\n",
        "    return lddt_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APsTEH73c2t7"
      },
      "source": [
        "### Training SetUp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDmKl67Dc2t7"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "epochs=config['epochs']\n",
        "# cos_epoch=config['cos_epoch']\n",
        "cos_epoch=0\n",
        "\n",
        "\n",
        "best_loss=np.inf\n",
        "optimizer = torch.optim.Adam(model.parameters(), weight_decay=0.0, lr=config['learning_rate']) #no weight decay following AF\n",
        "\n",
        "batch_size=1\n",
        "\n",
        "#for cycle in range(2):\n",
        "\n",
        "criterion=torch.nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "#scaler = GradScaler()\n",
        "\n",
        "schedule=torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(epochs-cos_epoch)*len(train_loader)//batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3f0iTjyc2t8"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8s-gaOEmc2t8",
        "outputId": "5fb7d83a-606b-4aa1-877b-32d6fb6ba86a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Loss: 1.7904695272445679: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.11it/s]\n",
            "<ipython-input-237-8bdb3a72ff62>:84: FutureWarning:\n",
            "\n",
            "The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.4629976749420166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Loss: 1.8015655279159546: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.460308074951172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Loss: 1.8047804832458496: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.45858097076416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Loss: 1.7915139198303223: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.456367015838623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 Loss: 1.7953580617904663: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.4538633823394775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 Loss: 1.7889316082000732: 100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.451791524887085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 Loss: 1.7997839450836182: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.45019268989563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 Loss: 1.7939746379852295: 100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.449519634246826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 Loss: 1.7883073091506958: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.4495856761932373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 Loss: 1.7955527305603027: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.4504454135894775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 Loss: 1.7908759117126465: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.450075626373291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 Loss: 1.7847557067871094: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.448533535003662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13 Loss: 1.7719858884811401: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.44669246673584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 Loss: 1.7680602073669434: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.445756435394287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 Loss: 1.7697553634643555: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.4444780349731445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16 Loss: 1.7653493881225586: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.4435269832611084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17 Loss: 1.7562270164489746: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.4427361488342285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18 Loss: 1.746379017829895: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.4402456283569336\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19 Loss: 1.7865328788757324: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.437685012817383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20 Loss: 1.7595462799072266: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.43430495262146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21 Loss: 1.7659273147583008: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.430065870285034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22 Loss: 1.7695096731185913: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.4263079166412354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23 Loss: 1.7540922164916992: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.422123908996582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24 Loss: 1.7473466396331787: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.4174444675445557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25 Loss: 1.7335705757141113: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.413233995437622\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26 Loss: 1.7469279766082764: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.4089882373809814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27 Loss: 1.7340044975280762: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.404493808746338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28 Loss: 1.669268250465393: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.4000484943389893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29 Loss: 1.6987203359603882: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.39547061920166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30 Loss: 1.6879661083221436: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.390557050704956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31 Loss: 1.6977711915969849: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.3861632347106934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32 Loss: 1.6965217590332031: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.3830347061157227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33 Loss: 1.641868233680725: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.379650115966797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34 Loss: 1.6933892965316772: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.3761074542999268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35 Loss: 1.665663242340088: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.372830390930176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36 Loss: 1.645926594734192: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.3690366744995117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37 Loss: 1.5995373725891113: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.366209030151367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38 Loss: 1.6560109853744507: 100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.3636980056762695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39 Loss: 1.5940635204315186: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.3615775108337402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40 Loss: 1.6197618246078491: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.3601255416870117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41 Loss: 1.6156129837036133: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.3590869903564453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42 Loss: 1.6110918521881104: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.358269214630127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43 Loss: 1.5965704917907715: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.3575634956359863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44 Loss: 1.6149322986602783: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.357110023498535\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45 Loss: 1.6233247518539429: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.3567376136779785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46 Loss: 1.6374107599258423: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.356426239013672\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47 Loss: 1.600538730621338: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.3562464714050293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48 Loss: 1.5816577672958374: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.3561272621154785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49 Loss: 1.5690792798995972: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.3560667037963867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000],\n",
            "        [-0.0203, -0.5614, -0.1539],\n",
            "        [-0.1074,  0.2944,  0.8067],\n",
            "        [ 1.3420,  1.6670,  1.6438],\n",
            "        [ 0.0341, -0.7036,  0.5366],\n",
            "        [ 1.2859,  0.9763,  2.1010],\n",
            "        [-0.3677, -0.4877,  0.8381],\n",
            "        [ 1.7884,  1.8754,  2.4167],\n",
            "        [ 1.4266,  1.5372,  2.5281],\n",
            "        [ 2.3883,  1.8390,  2.1705],\n",
            "        [ 1.2334,  1.1832,  1.9712],\n",
            "        [ 2.6285,  1.8483,  1.9626]], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "tensor([[  0.0000,   0.0000,   0.0000],\n",
            "        [ -1.0760,  -3.3350,  -3.4670],\n",
            "        [ -4.4130,  -6.6660,  -7.6410],\n",
            "        [ -7.0260,  -0.6310,  -9.7380],\n",
            "        [ -3.1140,   3.6950, -11.0880],\n",
            "        [ -2.5960,   8.3430, -14.4300],\n",
            "        [  1.2760,  10.8900, -15.9760],\n",
            "        [  7.1280,  12.5880, -19.2280],\n",
            "        [  8.5000,   9.1200, -14.2150],\n",
            "        [  4.5950,   6.0570,  -8.3870],\n",
            "        [  6.2800,   5.2940,  -2.2990],\n",
            "        [  6.1480,   2.7090,   3.4880]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50 Loss: 1.597622275352478: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.0294e-03, -3.4233e-03, -4.1739e-03],\n",
            "        [ 1.7007e+00,  1.9879e+00,  8.8714e-01],\n",
            "        [-3.3333e-02,  1.8321e-01, -3.3374e-01],\n",
            "        [-3.0462e-02,  1.7849e-01, -3.2829e-01],\n",
            "        [ 1.5612e+00,  1.3867e+00,  9.6201e-01],\n",
            "        [ 1.7058e+00,  1.9888e+00,  8.8780e-01],\n",
            "        [-3.1648e-02,  1.8571e-01, -3.3299e-01],\n",
            "        [ 1.5685e+00,  1.3868e+00,  9.6611e-01],\n",
            "        [ 1.7106e+00,  1.9841e+00,  8.9008e-01],\n",
            "        [-3.1808e-02,  1.8268e-01, -3.3836e-01],\n",
            "        [ 1.6387e-03, -4.4628e-03, -2.2395e-03],\n",
            "        [ 1.7140e+00,  1.9871e+00,  8.8825e-01],\n",
            "        [ 5.0555e-03, -1.1941e-03,  3.2316e-03],\n",
            "        [-3.4204e-02,  1.8508e-01, -3.2536e-01],\n",
            "        [ 7.8284e-03, -4.6958e-03, -6.6966e-03],\n",
            "        [-3.4074e-02,  1.8470e-01, -3.3213e-01],\n",
            "        [-3.6754e-02,  1.8024e-01, -3.3602e-01],\n",
            "        [ 1.7098e+00,  1.9694e+00,  8.9321e-01],\n",
            "        [ 1.5626e+00,  1.3807e+00,  9.5211e-01],\n",
            "        [ 1.5687e+00,  1.3739e+00,  9.4584e-01],\n",
            "        [ 1.6626e-02, -2.8447e-03,  6.4009e-04],\n",
            "        [-2.9601e-02,  1.7861e-01, -3.3847e-01],\n",
            "        [ 1.7248e+00,  1.9861e+00,  8.9488e-01],\n",
            "        [-3.2999e-02,  1.8370e-01, -3.3530e-01],\n",
            "        [ 1.5609e+00,  1.3887e+00,  9.5018e-01],\n",
            "        [-3.8050e-02,  1.8478e-01, -3.3630e-01],\n",
            "        [-3.6052e-02,  1.8521e-01, -3.4531e-01],\n",
            "        [ 1.1475e-02, -7.7379e-04, -7.7894e-03],\n",
            "        [ 1.5726e-02, -1.4369e-03, -5.8869e-03],\n",
            "        [-3.9474e-02,  1.9037e-01, -3.3640e-01],\n",
            "        [ 8.9414e-03, -4.2204e-03, -4.6019e-03],\n",
            "        [-3.0567e-02,  1.8171e-01, -3.4543e-01],\n",
            "        [-3.1725e-02,  1.8664e-01, -3.4023e-01],\n",
            "        [ 1.7117e+00,  1.9795e+00,  9.0122e-01],\n",
            "        [-3.3089e-02,  1.9460e-01, -3.3780e-01],\n",
            "        [ 1.1189e-02,  7.4887e-04, -4.4979e-03],\n",
            "        [ 8.2712e-03, -2.9104e-03, -7.6460e-03],\n",
            "        [ 1.0494e-02, -9.1219e-04, -1.1507e-02],\n",
            "        [ 1.3979e-02, -3.7774e-03, -1.0370e-02],\n",
            "        [ 1.6295e-02, -7.3326e-04, -1.0867e-02],\n",
            "        [ 1.0582e-02, -2.2444e-03, -6.7049e-03],\n",
            "        [ 1.5568e+00,  1.3962e+00,  9.4686e-01],\n",
            "        [-2.9292e-02,  1.9585e-01, -3.4425e-01],\n",
            "        [-3.3887e-02,  1.9239e-01, -3.4066e-01]], device='cuda:0')\n",
            "tensor([[  0.0000,   0.0000,   0.0000],\n",
            "        [ -0.7070,  -5.9580,   0.2510],\n",
            "        [  1.8180, -11.1220,   2.6320],\n",
            "        [  4.2810, -13.9570,   5.7980],\n",
            "        [  8.0630, -12.8990,  10.3460],\n",
            "        [ 10.0480, -10.8730,  15.0080],\n",
            "        [ 14.9250, -12.3670,  21.8680],\n",
            "        [ 12.1820,  -4.7690,  20.6990],\n",
            "        [  8.0700,  -1.0030,  22.7590],\n",
            "        [  1.6860,  -1.4760,  26.4590],\n",
            "        [  0.2910,   3.8560,  22.3290],\n",
            "        [ -4.6130,   2.8170,  21.1950],\n",
            "        [ -8.9810,  -0.8830,  22.1090],\n",
            "        [-10.0990,  -5.0790,  25.4830],\n",
            "        [-10.1600,  -7.7970,  30.2510],\n",
            "        [ -9.6060,  -8.0900,  35.5960],\n",
            "        [ -7.6790,  -6.0100,  40.9080],\n",
            "        [ -7.2770,  -1.9900,  44.3900],\n",
            "        [ -8.9710,   2.7920,  45.7980],\n",
            "        [-12.9290,   6.6020,  45.0960],\n",
            "        [-20.1510,   8.3560,  46.8390],\n",
            "        [-17.4410,  12.4890,  44.2450],\n",
            "        [-21.8030,  19.2040,  41.7060],\n",
            "        [-19.4020,  19.0910,  36.8900],\n",
            "        [-19.7190,  21.6050,  31.9250],\n",
            "        [-10.8560,  24.1070,  30.1850],\n",
            "        [ -7.3700,  16.5360,  29.4260],\n",
            "        [ -5.5970,  13.7800,  33.5650],\n",
            "        [ -6.2600,   8.0210,  40.2460],\n",
            "        [ -3.0390,   3.7300,  40.8050],\n",
            "        [ -1.7020,  -0.9740,  39.5490],\n",
            "        [ -1.1780,  -6.4040,  36.9250],\n",
            "        [ -2.3910,  -9.0850,  32.1110],\n",
            "        [ -3.3590,  -8.3700,  26.7530],\n",
            "        [ -6.9840, -11.3870,  22.3600],\n",
            "        [ -7.1440,  -8.1660,  18.1260],\n",
            "        [ -4.9090,  -2.5630,  16.6210],\n",
            "        [ -0.4550,   0.9240,  16.4910],\n",
            "        [  5.5070,   2.2800,  16.8240],\n",
            "        [ 10.2920,   0.3150,  15.6880],\n",
            "        [ 12.8350,  -3.7520,  13.0430],\n",
            "        [ 13.6350,  -7.6020,   9.2060],\n",
            "        [ 12.1330, -11.0210,   4.0290],\n",
            "        [  9.4680, -11.2910,  -0.5540],\n",
            "        [  5.5820,  -8.8450,  -4.4550]], device='cuda:0')\n",
            "val loss: 3.356048822402954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "save complete\n"
          ]
        }
      ],
      "source": [
        "version_name = 'triangle_test_set_1'\n",
        "\n",
        "best_val_loss=99999999999\n",
        "loss_df = pd.DataFrame(columns=['epoch', 'train_loss', 'val_loss'])\n",
        "\n",
        "# print(type(train_loader))\n",
        "\n",
        "# epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    tbar=tqdm(train_loader)\n",
        "    total_loss=0\n",
        "    train_loss=0\n",
        "\n",
        "    for idx, batch in enumerate(tbar):\n",
        "        #try:\n",
        "        sequence=batch['sequence'].cuda()\n",
        "        gt_xyz=batch['xyz'].squeeze().cuda()\n",
        "\n",
        "        mask=~torch.isnan(gt_xyz)\n",
        "        gt_xyz[torch.isnan(gt_xyz)]=0\n",
        "        # print('start sequence',sequence.shape)\n",
        "\n",
        "        pred_xyz = model(sequence).squeeze()\n",
        "        if epoch == epochs-1:\n",
        "              print(pred_xyz)\n",
        "              print(gt_xyz)\n",
        "\n",
        "        loss = dRMAE(pred_xyz,pred_xyz,gt_xyz,gt_xyz) + align_svd_mae(pred_xyz, gt_xyz)\n",
        "\n",
        "        if loss!=loss:\n",
        "            stop\n",
        "\n",
        "        try:\n",
        "          (loss/batch_size).backward()\n",
        "        except:\n",
        "          print(gt_xyz.shape)\n",
        "\n",
        "        if (idx+1)%batch_size==0 or idx+1 == len(tbar):\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            # scaler.scale(loss/batch_size).backward()\n",
        "            # scaler.unscale_(optimizer)\n",
        "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "            # scaler.step(optimizer)\n",
        "            # scaler.update()\n",
        "\n",
        "            if (epoch+1)>cos_epoch:\n",
        "                schedule.step()\n",
        "\n",
        "        total_loss += loss\n",
        "        train_loss = total_loss/(idx+1)\n",
        "\n",
        "        tbar.set_description(f\"Epoch {epoch + 1} Loss: {total_loss/(idx+1)}\")\n",
        "\n",
        "    ### Validation\n",
        "    tbar=tqdm(val_loader)\n",
        "\n",
        "    model.eval()\n",
        "    val_preds=[]\n",
        "    val_loss=0\n",
        "\n",
        "    for idx, batch in enumerate(tbar):\n",
        "        sequence=batch['sequence'].cuda()\n",
        "        gt_xyz=batch['xyz'].squeeze().cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred_xyz=model(sequence).squeeze()\n",
        "            loss = dRMAE(pred_xyz,pred_xyz,gt_xyz,gt_xyz) + align_svd_mae(pred_xyz, gt_xyz)\n",
        "            if epoch == epochs-1:\n",
        "              print(pred_xyz)\n",
        "              print(gt_xyz)\n",
        "\n",
        "        val_loss+=loss\n",
        "        val_preds.append([gt_xyz.cpu().numpy(),pred_xyz.cpu().numpy()])\n",
        "\n",
        "    val_loss=val_loss/len(tbar)\n",
        "    print(f\"val loss: {val_loss}\")\n",
        "\n",
        "    save_df = pd.DataFrame({'epoch': [epoch], 'train_loss': [train_loss.cpu().item()], 'val_loss': [val_loss.cpu().item()]})\n",
        "    loss_df = pd.concat([loss_df, save_df], ignore_index=True)\n",
        "\n",
        "    ## Check Best Loss .pt and Save\n",
        "    if val_loss<best_val_loss:\n",
        "        best_val_loss=val_loss\n",
        "        best_preds=val_preds\n",
        "        torch.save(model.state_dict(),f'/content/drive/MyDrive/RNA/WuSubSol/Save_Data/{version_name}_{epochs}_best.pt')\n",
        "\n",
        "    # 1.053595052265986 train loss after epoch 0\n",
        "torch.save(model.state_dict(),f'/content/drive/MyDrive/RNA/WuSubSol/Save_Data/{version_name}_{epochs}_last.pt')\n",
        "\n",
        "# Save Loss,\n",
        "\n",
        "loss_df.to_csv(f'/content/drive/MyDrive/RNA/WuSubSol/Save_Data/{version_name}_{epochs}_loss.csv', index=False)\n",
        "\n",
        "print('save complete')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Loss Graph"
      ],
      "metadata": {
        "id": "CFNZ0biDYkXV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyEnI8BYc2t9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "f345149f-ac40-4472-ca4d-770db9be9944"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"957d3709-9f6c-47d9-b512-e0e982109419\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"957d3709-9f6c-47d9-b512-e0e982109419\")) {                    Plotly.newPlot(                        \"957d3709-9f6c-47d9-b512-e0e982109419\",                        [{\"hovertemplate\":\"Epoch: %{x}\\u003cbr\\u003eTrain Loss: %{y:.4f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"line\":{\"color\":\"blue\"},\"mode\":\"lines+markers\",\"name\":\"Train Loss\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[1.790469527244568,1.8015655279159544,1.8047804832458496,1.7915139198303225,1.7953580617904663,1.7889316082000732,1.7997839450836182,1.7939746379852295,1.7883073091506958,1.7955527305603027,1.7908759117126465,1.7847557067871094,1.77198588848114,1.7680602073669434,1.7697553634643557,1.7653493881225586,1.7562270164489746,1.746379017829895,1.7865328788757324,1.7595462799072266,1.7659273147583008,1.7695096731185913,1.7540922164916992,1.7473466396331787,1.733570575714111,1.7469279766082764,1.7340044975280762,1.669268250465393,1.6987203359603882,1.6879661083221436,1.6977711915969849,1.6965217590332031,1.641868233680725,1.6933892965316772,1.665663242340088,1.645926594734192,1.599537372589111,1.656010985374451,1.5940635204315186,1.6197618246078491,1.6156129837036133,1.6110918521881104,1.5965704917907717,1.6149322986602783,1.6233247518539429,1.6374107599258425,1.600538730621338,1.5816577672958374,1.5690792798995972,1.597622275352478],\"type\":\"scatter\"},{\"hovertemplate\":\"Epoch: %{x}\\u003cbr\\u003eVal Loss: %{y:.4f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"line\":{\"color\":\"red\"},\"mode\":\"lines+markers\",\"name\":\"Val Loss\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[3.4629976749420166,3.460308074951172,3.45858097076416,3.456367015838623,3.453863382339477,3.451791524887085,3.45019268989563,3.449519634246826,3.4495856761932373,3.450445413589477,3.450075626373291,3.448533535003662,3.44669246673584,3.445756435394287,3.4444780349731445,3.4435269832611084,3.4427361488342285,3.4402456283569336,3.437685012817383,3.43430495262146,3.430065870285034,3.426307916641236,3.422123908996582,3.417444467544556,3.413233995437622,3.408988237380981,3.404493808746338,3.4000484943389893,3.39547061920166,3.390557050704956,3.3861632347106934,3.3830347061157227,3.379650115966797,3.3761074542999268,3.372830390930176,3.369036674499512,3.366209030151367,3.363698005676269,3.36157751083374,3.360125541687012,3.3590869903564453,3.358269214630127,3.3575634956359863,3.357110023498535,3.3567376136779785,3.356426239013672,3.3562464714050293,3.3561272621154785,3.3560667037963867,3.356048822402954],\"type\":\"scatter\"},{\"marker\":{\"color\":\"green\",\"size\":10},\"mode\":\"markers+text\",\"name\":\"Min Val\",\"text\":[\"Min Val Loss: 3.3560 (Epoch 49)\"],\"textposition\":\"top center\",\"x\":[49],\"y\":[3.356048822402954],\"type\":\"scatter\"},{\"marker\":{\"color\":\"green\",\"size\":10},\"mode\":\"markers+text\",\"name\":\"Min Train\",\"text\":[\"Min Train Loss: 1.5691 (Epoch 48)\"],\"textposition\":\"top center\",\"x\":[48],\"y\":[1.5690792798995972],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"legend\":{\"x\":0.01,\"y\":0.99},\"title\":{\"text\":\"triangle_test_set_1 - Train & Validation Loss : lr=1e-05_nlayers=48\"},\"xaxis\":{\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}},\"hovermode\":\"x unified\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('957d3709-9f6c-47d9-b512-e0e982109419');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "epochs=config['epochs']\n",
        "# CSV 로드\n",
        "df = pd.read_csv(f'/content/drive/MyDrive/RNA/WuSubSol/Save_Data/{version_name}_{epochs}_loss.csv')\n",
        "# df = pd.read_csv\n",
        "epochs = df['epoch']\n",
        "train_loss = df['train_loss']\n",
        "val_loss = df['val_loss']\n",
        "\n",
        "# 최솟값 정보\n",
        "min_train_loss = train_loss.min()\n",
        "min_train_epoch = epochs[train_loss.idxmin()]\n",
        "min_val_loss = val_loss.min()\n",
        "min_val_epoch = epochs[val_loss.idxmin()]\n",
        "\n",
        "# 그래프 만들기\n",
        "fig = go.Figure()\n",
        "\n",
        "# Train Loss\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=epochs,\n",
        "    y=train_loss,\n",
        "    mode='lines+markers',\n",
        "    name='Train Loss',\n",
        "    line=dict(color='blue'),\n",
        "    hovertemplate='Epoch: %{x}<br>Train Loss: %{y:.4f}<extra></extra>'\n",
        "))\n",
        "\n",
        "# Val Loss\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=epochs,\n",
        "    y=val_loss,\n",
        "    mode='lines+markers',\n",
        "    name='Val Loss',\n",
        "    line=dict(color='red'),\n",
        "    hovertemplate='Epoch: %{x}<br>Val Loss: %{y:.4f}<extra></extra>'\n",
        "))\n",
        "\n",
        "# 최소값 표시\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=[min_val_epoch],\n",
        "    y=[min_val_loss],\n",
        "    mode='markers+text',\n",
        "    marker=dict(color='green', size=10),\n",
        "    text=[f\"Min Val Loss: {min_val_loss:.4f} (Epoch {min_val_epoch})\"],\n",
        "    textposition='top center',\n",
        "    name='Min Val',\n",
        "\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=[min_train_epoch],\n",
        "    y=[min_train_loss],\n",
        "    mode='markers+text',\n",
        "    marker=dict(color='green', size=10),\n",
        "    text=[f\"Min Train Loss: {min_train_loss:.4f} (Epoch {min_train_epoch})\"],\n",
        "    textposition='top center',\n",
        "    name='Min Train',\n",
        "))\n",
        "# 레이아웃 꾸미기\n",
        "fig.update_layout(\n",
        "    title=f\"{version_name} - Train & Validation Loss : lr={config['learning_rate']}_nlayers={config['n_layers']}\",\n",
        "    xaxis_title=\"Epoch\",\n",
        "    yaxis_title=\"Loss\",\n",
        "    hovermode='x unified',\n",
        "    template='plotly_white',\n",
        "    legend=dict(x=0.01, y=0.99)\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bBpDtH1_R5-E"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "17wCulJ0c2tu"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}