{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DougChul/RNA/blob/Colab/Most_Basic_Structure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5-trPEoGHlh",
        "outputId": "720ae639-1feb-4a0d-e5b5-807157ab56a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozyTvAptc2tY"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR5zVYDac2td"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#set seed for everything\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "random.seed(0)"
      ],
      "metadata": {
        "id": "ggPH5fLOgMCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud1AE0c3c2te",
        "outputId": "46aa3fe6-62a0-403a-83f9-0026de8820f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n",
            "2.2.2\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)\n",
        "print(pd.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Quozo9asc2th"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdapDbGTc2ti"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"seed\": 0,\n",
        "    \"cutoff_date\": \"2020-01-01\",\n",
        "    \"test_cutoff_date\": \"2022-05-01\",\n",
        "    \"max_len\": 384,  ##\n",
        "    \"batch_size\": 1,\n",
        "    \"learning_rate\": 1e-5,\n",
        "    \"weight_decay\": 0.0,\n",
        "    \"mixed_precision\": \"bf16\",\n",
        "    \"model_config_path\": \"../working/configs/pairwise.yaml\",  # Adjust path as needed\n",
        "    \"epochs\": 50,  ##\n",
        "    \"loss_power_scale\": 1.0,\n",
        "    \"max_cycles\": 1,\n",
        "    \"grad_clip\": 0.1,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"d_clamp\": 30,\n",
        "    \"max_len_filter\": 384,\n",
        "    \"min_len_filter\": 10,\n",
        "    \"structural_violation_epoch\": 50,\n",
        "    \"balance_weight\": False,\n",
        "    \"n_tokens\": 4,\n",
        "    \"d_model\": 256,  ##\n",
        "    \"n_heads\": 8,\n",
        "    \"dropout\": 0.1,\n",
        "    \"d_ff\": 1024, ##\n",
        "    \"norm_ratio\": 0.3, ##\n",
        "    \"n_layers\": 30 ##\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Tanb_d5c2tk"
      },
      "source": [
        "## Set Sample Data To make Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aC6yuPPcc2tl",
        "outputId": "8945d6e4-f78f-40eb-d46a-ff077550a48e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8H4DW11kc2tl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# folder_path = '/content/drive/MyDrive/RNA/stanford-rna-3d-folding'\n",
        "\n",
        "# ## Select Data\n",
        "# # set_data_fomat = '5models'\n",
        "# set_data_fomat = 'v2'\n",
        "# # set_data_fomat = 'v1'\n",
        "\n",
        "# if set_data_fomat == '5models':\n",
        "#     train_labels = pd.read_csv(os.path.join(folder_path, 'pdb_labels_5models.csv'))\n",
        "#     train_sequences = pd.read_csv(os.path.join(folder_path, 'pdb_sequences_5models.csv'))\n",
        "# elif set_data_fomat == 'v2':\n",
        "#     train_labels = pd.read_csv(os.path.join(folder_path, 'train_labels.v2.csv'))\n",
        "#     train_sequences = pd.read_csv(os.path.join(folder_path, 'train_sequences.v2.csv'))\n",
        "# elif set_data_fomat == 'v1':\n",
        "#     train_labels = pd.read_csv(os.path.join(folder_path, 'train_labels.csv'))\n",
        "#     train_sequences = pd.read_csv(os.path.join(folder_path, 'train_sequences.csv'))\n",
        "# else:\n",
        "#     raise ValueError(\"Invalid set_data_fomat\")\n",
        "\n",
        "\n",
        "# print(train_labels.head())\n",
        "\n",
        "# print(train_sequences.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tef5mNWDc2tm"
      },
      "outputs": [],
      "source": [
        "# train_labels[\"pdb_id\"] = train_labels[\"ID\"].apply(lambda x: x.split(\"_\")[0]+'_'+x.split(\"_\")[1])\n",
        "# # train_sequences[\"pdb_id\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpqzHHsqc2tm"
      },
      "outputs": [],
      "source": [
        "# all_xyz=[]\n",
        "\n",
        "# test_sample = False\n",
        "# count = 0\n",
        "\n",
        "# for pdb_id in tqdm(train_sequences['target_id']):\n",
        "#     df = train_labels[train_labels[\"pdb_id\"]==pdb_id]\n",
        "#     #break\n",
        "#     # xyz=df[['x_1','y_1','z_1','x_2','y_2','z_2','x_3','y_3','z_3','x_4','y_4','z_4','x_5','y_5','z_5',]].to_numpy().astype('float32')\n",
        "#     xyz=df[['x_1','y_1','z_1']].to_numpy().astype('float32')\n",
        "#     xyz[xyz<-1e17]=float('Nan');\n",
        "#     all_xyz.append(xyz)\n",
        "#     if test_sample == True:\n",
        "#       count += 1\n",
        "#       if count == 100:\n",
        "#         break\n",
        "# # all_xyz[13]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V2OfrEec2tm"
      },
      "source": [
        "### filter data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "am-8hZfmc2tn"
      },
      "outputs": [],
      "source": [
        "# # filter the data\n",
        "# # Filter and process data\n",
        "# filter_nan = []\n",
        "# max_len = 0\n",
        "# filter_ratio = 0 # All data are valid\n",
        "# for xyz in all_xyz:\n",
        "#     if len(xyz) > max_len:\n",
        "#         max_len = len(xyz)\n",
        "\n",
        "#     filter_nan.append((np.isnan(xyz).mean() <= filter_ratio) & \\\n",
        "#                       (len(xyz)<config['max_len_filter']) & \\\n",
        "#                       (len(xyz)>config['min_len_filter']))\n",
        "\n",
        "\n",
        "# print(f\"Longest sequence in train: {max_len}\")\n",
        "\n",
        "# filter_nan = np.array(filter_nan)\n",
        "# non_nan_indices = np.arange(len(filter_nan))[filter_nan]\n",
        "# print('remain sequences:', len(non_nan_indices))\n",
        "# train_sequences = train_sequences.loc[non_nan_indices].reset_index(drop=True)\n",
        "# non_nan_xyz=[all_xyz[i] for i in non_nan_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ooxt9tkzc2to"
      },
      "outputs": [],
      "source": [
        "# #pack data into a dictionary\n",
        "\n",
        "# data={\n",
        "#       \"pdb_id\":train_sequences['target_id'].to_list(),\n",
        "#       \"sequence\":train_sequences['sequence'].to_list(),\n",
        "#       \"temporal_cutoff\": train_sequences['temporal_cutoff'].to_list(),\n",
        "#       \"description\": train_sequences['description'].to_list(),\n",
        "#       \"all_sequences\": train_sequences['all_sequences'].to_list(),\n",
        "#       \"xyz\": non_nan_xyz\n",
        "# }\n",
        "# print(data['pdb_id'][2])\n",
        "# print(data['sequence'][1])\n",
        "# print(data['temporal_cutoff'][1])\n",
        "# # print(data['description'][1])\n",
        "# # print(data['all_sequences'][1])\n",
        "# # print(data['xyz'][1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('/content/drive/MyDrive/RNA/stanford-rna-3d-folding/processed_data.v2_max=384.pkl', 'wb') as f:\n",
        "#     pickle.dump(data, f)\n",
        "# print(\"데이터가 processed_data.pkl 파일로 저장되었습니다.\")"
      ],
      "metadata": {
        "id": "I3fj1aWcIBVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load Preprocess DAta\n",
        "\n",
        "with open('/content/drive/MyDrive/RNA/stanford-rna-3d-folding/processed_data.v2_max=384.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)"
      ],
      "metadata": {
        "id": "7LfEXiYJPGdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTOEWj85c2tp"
      },
      "source": [
        "### Split Train / Validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZYN3QxSc2tq"
      },
      "outputs": [],
      "source": [
        "# Split data into train and test\n",
        "all_index = np.arange(len(data['sequence']))\n",
        "cutoff_date = pd.Timestamp(config['cutoff_date'])\n",
        "test_cutoff_date = pd.Timestamp(config['test_cutoff_date'])\n",
        "train_index = [i for i, d in enumerate(data['temporal_cutoff']) if pd.Timestamp(d) <= cutoff_date]\n",
        "test_index = [i for i, d in enumerate(data['temporal_cutoff']) if pd.Timestamp(d) > cutoff_date and pd.Timestamp(d) <= test_cutoff_date]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rmby3eM5c2tq"
      },
      "source": [
        "### To Pytorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBbFjh1Xc2tq"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from ast import literal_eval\n",
        "\n",
        "def get_ct(bp,s):\n",
        "    ct_matrix=np.zeros((len(s),len(s)))\n",
        "    for b in bp:\n",
        "        ct_matrix[b[0]-1,b[1]-1]=1\n",
        "    return ct_matrix\n",
        "\n",
        "class RNA3D_Dataset(Dataset):\n",
        "    def __init__(self,indices,data):\n",
        "        self.indices=indices\n",
        "        self.data=data\n",
        "        self.tokens={nt:i for i,nt in enumerate('ACGU')}\n",
        "        # self.tokens = {'A': 2, 'U':-2, 'G':3, 'C':-3}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        idx=self.indices[idx]\n",
        "        sequence=[self.tokens[nt] for nt in (self.data['sequence'][idx])]\n",
        "        sequence=np.array(sequence)\n",
        "        sequence=torch.tensor(sequence)\n",
        "\n",
        "        #get C1' xyz\n",
        "        xyz=self.data['xyz'][idx]\n",
        "        xyz=torch.tensor(np.array(xyz))\n",
        "\n",
        "\n",
        "        if len(sequence)>config['max_len']:\n",
        "            crop_start=np.random.randint(len(sequence)-config['max_len'])\n",
        "            crop_end=crop_start+config['max_len']\n",
        "\n",
        "            sequence=sequence[crop_start:crop_end]\n",
        "            xyz=xyz[crop_start:crop_end]\n",
        "        #center at first atom if first atom does not exit go until it does\n",
        "        for i in range(len(xyz)):\n",
        "            if (~torch.isnan(xyz[i])).all():\n",
        "                break\n",
        "        xyz=xyz-xyz[i]\n",
        "\n",
        "        return {'pbd_id':self.data['pdb_id'][idx],\n",
        "            'sequence':sequence,\n",
        "                'xyz':xyz}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBsrRj10c2tr"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_dataset=RNA3D_Dataset(train_index,data)\n",
        "val_dataset=RNA3D_Dataset(test_index,data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Whap4bhc2ts"
      },
      "outputs": [],
      "source": [
        "train_loader=DataLoader(train_dataset,batch_size=1,shuffle=True)\n",
        "val_loader=DataLoader(val_dataset,batch_size=1,shuffle=False)\n",
        "\n",
        "# print(train_dataset[2]['xyz'][6])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyKuOInqc2ts"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y1zf5eAc2tt"
      },
      "source": [
        "### Example with one data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvdEweIFc2tt",
        "outputId": "4ed0f958-316f-44cb-b054-748b07c0f580",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n",
            "{'pbd_id': '3J0Q_3', 'sequence': tensor([2, 1, 1, 0, 2, 3, 2, 0, 0, 0, 3, 0]), 'xyz': tensor([[  0.0000,   0.0000,   0.0000],\n",
            "        [ -1.0760,  -3.3350,  -3.4670],\n",
            "        [ -4.4130,  -6.6660,  -7.6410],\n",
            "        [ -7.0260,  -0.6310,  -9.7380],\n",
            "        [ -3.1140,   3.6950, -11.0880],\n",
            "        [ -2.5960,   8.3430, -14.4300],\n",
            "        [  1.2760,  10.8900, -15.9760],\n",
            "        [  7.1280,  12.5880, -19.2280],\n",
            "        [  8.5000,   9.1200, -14.2150],\n",
            "        [  4.5950,   6.0570,  -8.3870],\n",
            "        [  6.2800,   5.2940,  -2.2990],\n",
            "        [  6.1480,   2.7090,   3.4880]])}\n",
            "1\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "test_sample = True\n",
        "\n",
        "if test_sample == True:\n",
        "  from torch.utils.data import Subset\n",
        "\n",
        "  target_index = 0\n",
        "  train_dataset = Subset(train_loader.dataset, [target_index])\n",
        "  train_loader = DataLoader(train_dataset, batch_size=1)\n",
        "\n",
        "  val_dataset = Subset(val_loader.dataset, [target_index])\n",
        "  val_loader = DataLoader(val_dataset, batch_size=1)\n",
        "\n",
        "print(len(train_dataset[0]['sequence']))\n",
        "print(train_dataset[0])\n",
        "print(len(train_loader))\n",
        "print(len(val_loader))\n",
        "# print(f\"새로운 데이터셋 크기: {len(single_data_loader.dataset)}\")\n",
        "# print(f\"새로운 DataLoader의 배치 개수: {len(single_data_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17wCulJ0c2tu"
      },
      "source": [
        "### Check Pairwise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgYG2Xm-c2tu"
      },
      "outputs": [],
      "source": [
        "# import RNA\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# src = torch.tensor([2, 2, 1, 2, 3, 0, 0, 2, 2, 0, 3, 3, 0, 1, 1, 3, 0, 3, 2, 1, 1], dtype=torch.long)\n",
        "# # AUGC를 0123으로 매핑\n",
        "# nucleotide_map = {0: 'A', 1: 'C', 2: 'G', 3: 'U'}\n",
        "\n",
        "# str_A = ''.join([nucleotide_map[x.item()] for x in src])\n",
        "\n",
        "# print(len(str_A))\n",
        "# print(str_A)\n",
        "\n",
        "# md = RNA.md()\n",
        "\n",
        "# fc = RNA.fold_compound(str_A, md)\n",
        "\n",
        "# # predict Minmum Free Energy and corresponding secondary structure\n",
        "# # (ss, mfe) =\n",
        "# # print(fc.mfe())\n",
        "# fc.pf()\n",
        "# B = torch.tensor(fc.bpp())\n",
        "\n",
        "# # B = torch.tensor(B)\n",
        "# B = B[1:,1:]\n",
        "# print(type(B))\n",
        "# print(B.shape)\n",
        "# B = B + B.T\n",
        "# # B = np.array(B, dtype=np.bool())\n",
        "# # print(B)\n",
        "\n",
        "# plt.figure(figsize=(5,4))\n",
        "# sns.heatmap(B, cmap='viridis', annot=False)  # annot=True로 설정하면 각 셀에 값 표시\n",
        "# plt.title('Pairwise Contact Probability Matrix')\n",
        "# plt.show()\n",
        "\n",
        "# # def get_pairwise_features(src, seq_len, d_model):\n",
        "# #     nucleotide_map = {0: 'A', 1: 'C', 2: 'G', 3: 'U'}\n",
        "\n",
        "# #     # print(\"pairwise\",src)\n",
        "# #     src = src.squeeze(0)\n",
        "# #     str_seq = ''.join([nucleotide_map[x.item()] for x in src])\n",
        "# #     # print(\"str_seq\",str_seq)\n",
        "\n",
        "# #     md = RNA.md()\n",
        "# #     fc = RNA.fold_compound(str_seq, md)\n",
        "# #     fc.pf() ##???\n",
        "\n",
        "# #     pair_matrix = torch.tensor(fc.bpp(), dtype=torch.float32)\n",
        "# #     # print(\"pair_matrix\",pair_matrix)\n",
        "# #     pair_matrix = pair_matrix[1:,1:] # remove first row and column 0 index in bpp is always 0\n",
        "# #     pair_matrix = pair_matrix + pair_matrix.T # symmetric matrix\n",
        "\n",
        "# #     pair_matrix = pair_matrix.unsqueeze(0).unsqueeze(0)\n",
        "# #     pair_matrix = F.interpolate(pair_matrix, size=(d_model, d_model), mode='bilinear', align_corners=False)\n",
        "# #     # pair_matrix = F.interpolate(pair_matrix, size=(d_model, d_model), mode='nearest', align_corners=False)\n",
        "\n",
        "# #     pair_matrix = pair_matrix.squeeze()\n",
        "# #     # print(\"pair_matrix\",pair_matrix.shape)\n",
        "\n",
        "# #     return pair_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8ueTbF_c2tv"
      },
      "source": [
        "### MSA Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUYNiz6jc2tw"
      },
      "outputs": [],
      "source": [
        "class OuterProductMean(nn.Module):\n",
        "    def __init__(self, in_dim=256, dim_msa=32, pairwise_dim=128):\n",
        "        super(OuterProductMean, self).__init__()\n",
        "        self.proj_down1 = nn.Linear(in_dim, dim_msa)\n",
        "        self.proj_down2 = nn.Linear(dim_msa ** 2, pairwise_dim)\n",
        "\n",
        "    def forward(self,seq_rep, pair_rep=None):\n",
        "        seq_rep=self.proj_down1(seq_rep)\n",
        "        outer_product = torch.einsum('bid,bjc -> bijcd', seq_rep, seq_rep)\n",
        "        outer_product = rearrange(outer_product, 'b i j c d -> b i j (c d)')\n",
        "        outer_product = self.proj_down2(outer_product)\n",
        "\n",
        "        if pair_rep is not None:\n",
        "            outer_product=outer_product+pair_rep\n",
        "\n",
        "        return outer_product"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mrsxE-qc2tx"
      },
      "source": [
        "### Back Bone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHBhYHxEc2tx"
      },
      "outputs": [],
      "source": [
        "# import RNA\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat, reduce\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout, max_len=config['max_len']):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print('x', x.shape)\n",
        "        # print('pe', self.pe[:, : x.size(1)])\n",
        "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class relpos(nn.Module):\n",
        "\n",
        "    def __init__(self, dim=64):\n",
        "        super(relpos, self).__init__()\n",
        "        self.linear = nn.Linear(33, dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        L=src.shape[1]\n",
        "        res_id = torch.arange(L).to(src.device).unsqueeze(0)\n",
        "        device = res_id.device\n",
        "        bin_values = torch.arange(-16, 17, device=device)\n",
        "        #print((bin_values))\n",
        "        d = res_id[:, :, None] - res_id[:, None, :]\n",
        "        bdy = torch.tensor(16, device=device)\n",
        "        d = torch.minimum(torch.maximum(-bdy, d), bdy)\n",
        "        d_onehot = (d[..., None] == bin_values).float()\n",
        "        #print(d_onehot.sum(dim=-1).min())\n",
        "        assert d_onehot.sum(dim=-1).min() == 1\n",
        "        p = self.linear(d_onehot)\n",
        "        return p\n",
        "\n",
        "# class ConstrainedPositionalEncoding(nn.Module):\n",
        "#     def __init__(self, d_model, max_relative_position=32, constrained_position=4):\n",
        "#         super().__init__()\n",
        "#         self.max_relative_position = max_relative_position\n",
        "#         self.relative_embedding = nn.Embedding(2 * max_relative_position + 1, d_model)\n",
        "#         self.constrained_position = constrained_position\n",
        "\n",
        "#     def forward(self, seq_len):\n",
        "\n",
        "#         return pos_encoding(seq_len, self.d_model)\n",
        "\n",
        "\n",
        "class EmbedSequence(nn.Module):\n",
        "    def __init__(self, d_model, n_tokens = config['n_tokens']):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(n_tokens, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout=0.1)\n",
        "        # self.pos_encoding = PosEncoding(seq_len, d_model)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.embedding(src.long())\n",
        "        return src + self.pos_encoder(src)\n",
        "\n",
        "\n",
        "class MultiheadAtt(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=8, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_model // n_heads\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.w_q = nn.Linear(d_model, n_heads * self.d_head)\n",
        "        self.w_k = nn.Linear(d_model, n_heads * self.d_head)\n",
        "        self.w_v = nn.Linear(d_model, n_heads * self.d_head)\n",
        "\n",
        "        self.w_o = nn.Linear(n_heads * self.d_head, d_model)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        # print('query.shape',query.shape)\n",
        "        batch_size, length, d_model = query.size()\n",
        "\n",
        "        q = self.w_q(query)\n",
        "        k = self.w_k(key)\n",
        "        v = self.w_v(value)\n",
        "\n",
        "        q = q.view(batch_size, length, self.n_heads, self.d_head)\n",
        "        k = k.view(batch_size, length, self.n_heads, self.d_head)\n",
        "        v = v.view(batch_size, length, self.n_heads, self.d_head)\n",
        "\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
        "        scores = self.dropout(F.softmax(scores, dim=-1))\n",
        "\n",
        "        output = torch.matmul(scores, v)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, length, -1)\n",
        "        output = self.w_o(output)\n",
        "\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.w_1(x)\n",
        "        x = self.activation(x)\n",
        "        if self.dropout is not None:\n",
        "            x = self.dropout(x)\n",
        "        x = self.w_2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0, norm_type='post_ln'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_ff = config['d_ff']\n",
        "\n",
        "        if (norm_type == 'post_ln') or (norm_type == 'pre_ln'):\n",
        "            self.norm_type = norm_type\n",
        "        else:\n",
        "            raise ValueError(\"Invalid norm_type\")\n",
        "\n",
        "        self.norm_0 = nn.LayerNorm(d_model)\n",
        "        self.norm_1 = nn.LayerNorm(d_model)\n",
        "        self.norm_2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.attention = MultiheadAtt(d_model, n_heads, dropout)\n",
        "        self.feedforward = FeedForward(d_model, self.d_ff, dropout)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, src, pairwise_feature=None):\n",
        "        # src = residual_connection(src, self.encoder(src))\n",
        "        norm_type = self.norm_type\n",
        "\n",
        "\n",
        "        if norm_type == 'post_ln':\n",
        "\n",
        "            src = self.norm_0(src)\n",
        "            src = src + self.attention(src,src,src)  #residual conn\n",
        "            src = self.norm_1(src)\n",
        "            src = src + self.feedforward(src)  #residual conn\n",
        "            src = self.norm_2(src)\n",
        "\n",
        "\n",
        "        elif norm_type == 'pre_ln':\n",
        "            src_temp = src\n",
        "            src = self.norm_0(src)\n",
        "            src = self.attention(src,src,src) + src_temp\n",
        "            # src = self.activation(src)\n",
        "            src_temp = src\n",
        "            src = self.norm_1(src)\n",
        "            src = self.feedforward(src) + src_temp\n",
        "            # src = self.activation(src)\n",
        "            src = self.norm_2(src)\n",
        "\n",
        "        return src\n",
        "\n",
        "\n",
        "class WuSubSol(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(WuSubSol, self).__init__()\n",
        "        self.config = config\n",
        "        self.n_layers = config['n_layers']\n",
        "        self.d_model = config['d_model']\n",
        "\n",
        "        self.norm_ratio = config['norm_ratio']\n",
        "        if (self.n_layers * self.norm_ratio).is_integer():\n",
        "            self.n_post_ln = int(self.n_layers * self.norm_ratio)\n",
        "        else:\n",
        "            print(\"Invalid norm_ratio\")\n",
        "            exit()\n",
        "\n",
        "        self.embedding = EmbedSequence(self.d_model)\n",
        "\n",
        "\n",
        "        self.encoder_layers = []\n",
        "        count_post_ln, count_pre_ln = 0,0\n",
        "        for i in range(self.n_post_ln):\n",
        "            self.encoder_layers.append(EncoderLayer(self.d_model, config['n_heads'], config['dropout'], 'post_ln'))\n",
        "            count_post_ln += 1\n",
        "        for i in range(self.n_layers - self.n_post_ln):\n",
        "            self.encoder_layers.append(EncoderLayer(self.d_model, config['n_heads'], config['dropout'], 'pre_ln'))\n",
        "            count_pre_ln += 1\n",
        "\n",
        "        print(f\"{count_post_ln} post_ln layers and {count_pre_ln} pre_ln layers\")\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList(self.encoder_layers)\n",
        "\n",
        "        self.final_linear = nn.Linear(self.d_model, 3)\n",
        "\n",
        "        print(f\"{self.n_layers} layers of encoder constructed\")\n",
        "\n",
        "    def forward(self, src):\n",
        "        pairwise_feature = None\n",
        "        src = self.embedding(src) # L*d_model\n",
        "        # print('after embedding', src.shape)\n",
        "\n",
        "\n",
        "        for i,layer in enumerate(self.encoder_layers):\n",
        "            # print(\"before\",src.shape)\n",
        "            src = layer(src, pairwise_feature)\n",
        "            # print(\"after\",src.shape)\n",
        "\n",
        "        src = self.final_linear(src).squeeze()\n",
        "        for i in range(len(src)):\n",
        "            if (~torch.isnan(src[i])).all():\n",
        "                break\n",
        "        src=src-src[i]\n",
        "\n",
        "        # print('final tensor-Shape : ', src.shape)\n",
        "\n",
        "        return src\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV-uV_vHc2tz",
        "outputId": "9ea6b752-59f0-456d-d48f-374c21047f1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9 post_ln layers and 21 pre_ln layers\n",
            "30 layers of encoder constructed\n",
            "WuSubSol(\n",
            "  (embedding): EmbedSequence(\n",
            "    (embedding): Embedding(4, 256)\n",
            "    (pos_encoder): PositionalEncoding(\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (encoder_layers): ModuleList(\n",
            "    (0-29): 30 x EncoderLayer(\n",
            "      (norm_0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (attention): MultiheadAtt(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (w_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (w_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (w_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (w_o): Linear(in_features=256, out_features=256, bias=True)\n",
            "      )\n",
            "      (feedforward): FeedForward(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (w_1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (activation): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (final_linear): Linear(in_features=256, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = WuSubSol(config)\n",
        "model.to(device)\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_IUreJxc2t2"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJLQJrhEc2t5"
      },
      "source": [
        "### Define Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hKk0tdlc2t5"
      },
      "outputs": [],
      "source": [
        "def calculate_distance_matrix(X,Y,epsilon=1e-4):\n",
        "    return (torch.square(X[:,None]-Y[None,:])+epsilon).sum(-1).sqrt()\n",
        "\n",
        "def dRMAE(pred_x,\n",
        "          pred_y,\n",
        "          gt_x,\n",
        "          gt_y,\n",
        "          epsilon=1e-4,Z=10,d_clamp=None):\n",
        "    pred_dm=calculate_distance_matrix(pred_x,pred_y)\n",
        "    gt_dm=calculate_distance_matrix(gt_x,gt_y)\n",
        "\n",
        "    mask=~torch.isnan(gt_dm)\n",
        "    mask[torch.eye(mask.shape[0]).bool()]=False\n",
        "\n",
        "    rmsd=torch.abs(pred_dm[mask]-gt_dm[mask])\n",
        "\n",
        "    return rmsd.mean()/Z\n",
        "\n",
        "def align_svd_mae(input, target, Z=10):\n",
        "    \"\"\"\n",
        "    Aligns the input (Nx3) to target (Nx3) using SVD-based Procrustes alignment\n",
        "    and computes RMSD loss.\n",
        "\n",
        "    Args:\n",
        "        input (torch.Tensor): Nx3 tensor representing the input points.\n",
        "        target (torch.Tensor): Nx3 tensor representing the target points.\n",
        "\n",
        "    Returns:\n",
        "        aligned_input (torch.Tensor): Nx3 aligned input.\n",
        "        rmsd_loss (torch.Tensor): RMSD loss.\n",
        "    \"\"\"\n",
        "    # print('input-Shape', input.shape)\n",
        "    # print('output-Shape', target.shape)\n",
        "    # target = target[:, :3]\n",
        "    assert input.shape == target.shape, \"Input and target must have the same shape\"\n",
        "\n",
        "    #mask\n",
        "    mask=~torch.isnan(target.sum(-1))\n",
        "\n",
        "    input=input[mask]\n",
        "    target=target[mask]\n",
        "\n",
        "    # Compute centroids\n",
        "    centroid_input = input.mean(dim=0, keepdim=True)\n",
        "    centroid_target = target.mean(dim=0, keepdim=True)\n",
        "\n",
        "    # Center the points\n",
        "    input_centered = input - centroid_input.detach()\n",
        "    target_centered = target - centroid_target\n",
        "\n",
        "    # Compute covariance matrix\n",
        "    cov_matrix = input_centered.T @ target_centered\n",
        "\n",
        "    # SVD to find optimal rotation\n",
        "    U, S, Vt = torch.svd(cov_matrix)\n",
        "\n",
        "    # Compute rotation matrix\n",
        "    R = Vt @ U.T\n",
        "\n",
        "    # Ensure a proper rotation (det(R) = 1, no reflection)\n",
        "    if torch.det(R) < 0:\n",
        "        Vt[-1, :] *= -1\n",
        "        R = Vt @ U.T\n",
        "\n",
        "    # Rotate input\n",
        "    aligned_input = (input_centered @ R.T.detach()) + centroid_target.detach()\n",
        "\n",
        "    # # Compute RMSD loss\n",
        "    # rmsd_loss = torch.sqrt(((aligned_input - target) ** 2).mean())\n",
        "\n",
        "    # rmsd_loss = torch.sqrt(((aligned_input - target) ** 2).mean())\n",
        "\n",
        "    # return aligned_input, rmsd_loss\n",
        "    return torch.abs(aligned_input-target).mean()/Z\n",
        "\n",
        "def align_svd_rmsd(input, target):\n",
        "    \"\"\"\n",
        "    Aligns the input (Nx3) to target (Nx3) using SVD-based Procrustes alignment\n",
        "    and computes RMSD loss.\n",
        "\n",
        "    Args:\n",
        "        input (torch.Tensor): Nx3 tensor representing the input points.\n",
        "        target (torch.Tensor): Nx3 tensor representing the target points.\n",
        "\n",
        "    Returns:\n",
        "        aligned_input (torch.Tensor): Nx3 aligned input.\n",
        "        rmsd_loss (torch.Tensor): RMSD loss.\n",
        "    \"\"\"\n",
        "    # print('input-Shape', input.shape)\n",
        "    # print('output-Shape', target.shape)\n",
        "    # target = target[:,:3]\n",
        "    assert input.shape == target.shape, \"Input and target must have the same shape\"\n",
        "\n",
        "    #mask\n",
        "    mask=~torch.isnan(target.sum(-1))\n",
        "\n",
        "\n",
        "    input=input[mask]\n",
        "    target=target[mask]\n",
        "\n",
        "    # Compute centroids\n",
        "    centroid_input = input.mean(dim=0, keepdim=True)\n",
        "    centroid_target = target.mean(dim=0, keepdim=True)\n",
        "\n",
        "    # Center the points\n",
        "    input_centered = input - centroid_input.detach()\n",
        "    target_centered = target - centroid_target\n",
        "\n",
        "    # Compute covariance matrix\n",
        "    cov_matrix = input_centered.T @ target_centered\n",
        "\n",
        "    # SVD to find optimal rotation\n",
        "    U, S, Vt = torch.svd(cov_matrix)\n",
        "\n",
        "    # Compute rotation matrix\n",
        "    R = Vt @ U.T\n",
        "\n",
        "    # Ensure a proper rotation (det(R) = 1, no reflection)\n",
        "    if torch.det(R) < 0:\n",
        "        Vt[-1, :] *= -1\n",
        "        R = Vt @ U.T\n",
        "\n",
        "    # Rotate input\n",
        "    aligned_input = (input_centered @ R.T.detach()) + centroid_target.detach()\n",
        "\n",
        "    # # Compute RMSD loss\n",
        "    # rmsd_loss = torch.sqrt(((aligned_input - target) ** 2).mean())\n",
        "\n",
        "    # rmsd_loss = torch.sqrt(((aligned_input - target) ** 2).mean())\n",
        "\n",
        "    # return aligned_input, rmsd_loss\n",
        "    return torch.square(aligned_input-target).mean().sqrt()\n",
        "\n",
        "def compute_lddt(ground_truth_atoms, predicted_atoms, cutoff=30.0, thresholds=[1.0, 2.0, 4.0, 8.0]):\n",
        "    \"\"\"\n",
        "    Computes the lDDT score between ground truth and predicted atoms.\n",
        "\n",
        "    Parameters:\n",
        "        ground_truth_atoms (np.array): Nx3 array of ground truth atom coordinates.\n",
        "        predicted_atoms (np.array): Nx3 array of predicted atom coordinates.\n",
        "        cutoff (float): Distance cutoff in Ångstroms to consider neighbors. Default is 30 Å.\n",
        "        thresholds (list): List of thresholds in Ångstroms for the lDDT computation. Default is [0.5, 1.0, 2.0, 4.0].\n",
        "\n",
        "    Returns:\n",
        "        float: The lDDT score.\n",
        "    \"\"\"\n",
        "    # Number of atoms\n",
        "    num_atoms = ground_truth_atoms.shape[0]\n",
        "\n",
        "    # Initialize array to store lDDT fractions for each threshold\n",
        "    fractions = np.zeros(len(thresholds))\n",
        "\n",
        "    for i in range(num_atoms):\n",
        "        # Get the distances from atom i to all other atoms for both ground truth and predicted atoms\n",
        "        gt_distances = np.linalg.norm(ground_truth_atoms[i] - ground_truth_atoms, axis=1)\n",
        "        pred_distances = np.linalg.norm(predicted_atoms[i] - predicted_atoms, axis=1)\n",
        "\n",
        "        # print(gt_distances)\n",
        "        # print(pred_distances)\n",
        "        # exit()\n",
        "        # Apply the cutoff to consider only distances within the cutoff range\n",
        "        mask = (gt_distances > 0) & (gt_distances < cutoff)\n",
        "\n",
        "        # Calculate the absolute difference between ground truth and predicted distances\n",
        "        distance_diff = np.abs(gt_distances[mask] - pred_distances[mask])\n",
        "\n",
        "        # Filter out any NaN values from the distance difference calculation\n",
        "        valid_mask = ~np.isnan(distance_diff)\n",
        "        distance_diff = distance_diff[valid_mask]\n",
        "\n",
        "        # Compute the fractions for each threshold\n",
        "        for j, threshold in enumerate(thresholds):\n",
        "            if len(distance_diff)>0:\n",
        "                fractions[j] += np.mean(distance_diff < threshold)\n",
        "    # print(fractions)\n",
        "    # print(num_atoms)\n",
        "\n",
        "    # Average the fractions over the number of atoms\n",
        "    fractions /= num_atoms\n",
        "\n",
        "    # The final lDDT score is the average of these fractions\n",
        "    lddt_score = np.mean(fractions)\n",
        "\n",
        "    return lddt_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APsTEH73c2t7"
      },
      "source": [
        "### Training SetUp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDmKl67Dc2t7"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "epochs=config['epochs']\n",
        "# cos_epoch=config['cos_epoch']\n",
        "cos_epoch=0\n",
        "\n",
        "\n",
        "best_loss=np.inf\n",
        "optimizer = torch.optim.Adam(model.parameters(), weight_decay=0.0, lr=config['learning_rate']) #no weight decay following AF\n",
        "\n",
        "batch_size=1\n",
        "\n",
        "#for cycle in range(2):\n",
        "\n",
        "criterion=torch.nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "#scaler = GradScaler()\n",
        "\n",
        "schedule=torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(epochs-cos_epoch)*len(train_loader)//batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3f0iTjyc2t8"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8s-gaOEmc2t8",
        "outputId": "02d135c1-9476-4b4c-be8a-d44771b0e66a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Loss: 1.8013463020324707: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 36.99it/s]\n",
            "<ipython-input-25-ca257142f279>:86: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  loss_df = pd.concat([loss_df, save_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.4327447414398193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Loss: 1.7963980436325073: 100%|██████████| 1/1 [00:00<00:00, 11.32it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 40.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.4241013526916504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Loss: 1.7817728519439697: 100%|██████████| 1/1 [00:00<00:00,  9.78it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 40.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.41202974319458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Loss: 1.753955364227295: 100%|██████████| 1/1 [00:00<00:00, 10.99it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 39.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.3981881141662598\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 Loss: 1.7171471118927002: 100%|██████████| 1/1 [00:00<00:00,  9.58it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 39.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.3834710121154785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 Loss: 1.687034010887146: 100%|██████████| 1/1 [00:00<00:00, 11.19it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 41.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.368791341781616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 Loss: 1.6689987182617188: 100%|██████████| 1/1 [00:00<00:00, 11.50it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 40.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.3542368412017822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 Loss: 1.626583218574524: 100%|██████████| 1/1 [00:00<00:00,  8.25it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 37.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.340333938598633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 Loss: 1.5745735168457031: 100%|██████████| 1/1 [00:00<00:00, 11.42it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 42.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.3278942108154297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 Loss: 1.5800257921218872: 100%|██████████| 1/1 [00:00<00:00, 11.12it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 39.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.3192789554595947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 Loss: 1.5313220024108887: 100%|██████████| 1/1 [00:00<00:00, 11.45it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 40.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.314365863800049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 Loss: 1.5038418769836426: 100%|██████████| 1/1 [00:00<00:00, 11.28it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 40.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.3089089393615723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13 Loss: 1.483441710472107: 100%|██████████| 1/1 [00:00<00:00, 11.06it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 40.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.304979085922241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 Loss: 1.4734010696411133: 100%|██████████| 1/1 [00:00<00:00, 11.28it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 36.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.300119400024414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 Loss: 1.4664329290390015: 100%|██████████| 1/1 [00:00<00:00, 11.15it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 39.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.2946505546569824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16 Loss: 1.4174222946166992: 100%|██████████| 1/1 [00:00<00:00, 11.29it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 40.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.287490129470825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17 Loss: 1.4128222465515137: 100%|██████████| 1/1 [00:00<00:00, 11.15it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 40.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.2788898944854736\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18 Loss: 1.351910948753357: 100%|██████████| 1/1 [00:00<00:00, 11.39it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 41.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.268570899963379\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19 Loss: 1.3972586393356323: 100%|██████████| 1/1 [00:00<00:00, 10.49it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 41.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.257856845855713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20 Loss: 1.3639214038848877: 100%|██████████| 1/1 [00:00<00:00, 11.24it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 38.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.24704909324646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21 Loss: 1.3444607257843018: 100%|██████████| 1/1 [00:00<00:00, 10.30it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 28.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.238285541534424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22 Loss: 1.3016530275344849: 100%|██████████| 1/1 [00:00<00:00, 10.79it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 40.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.2303311824798584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23 Loss: 1.329411268234253: 100%|██████████| 1/1 [00:00<00:00, 11.40it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 40.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.2234694957733154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24 Loss: 1.3014200925827026: 100%|██████████| 1/1 [00:00<00:00, 10.84it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 40.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.2172346115112305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25 Loss: 1.3310610055923462: 100%|██████████| 1/1 [00:00<00:00, 11.10it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 40.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.2124931812286377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26 Loss: 1.2982301712036133: 100%|██████████| 1/1 [00:00<00:00, 11.28it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 40.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.2086055278778076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27 Loss: 1.3026700019836426: 100%|██████████| 1/1 [00:00<00:00, 11.54it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 41.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.20554780960083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28 Loss: 1.2981529235839844: 100%|██████████| 1/1 [00:00<00:00, 11.25it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 40.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.202108860015869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29 Loss: 1.2955529689788818: 100%|██████████| 1/1 [00:00<00:00, 10.92it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 41.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.199486494064331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30 Loss: 1.2875205278396606: 100%|██████████| 1/1 [00:00<00:00, 10.91it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 40.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.1972150802612305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31 Loss: 1.2926946878433228: 100%|██████████| 1/1 [00:00<00:00, 11.28it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 39.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.1948580741882324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32 Loss: 1.2682563066482544: 100%|██████████| 1/1 [00:00<00:00, 11.45it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 40.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.1916985511779785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33 Loss: 1.241859793663025: 100%|██████████| 1/1 [00:00<00:00, 11.23it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 39.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.188926935195923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34 Loss: 1.2788927555084229: 100%|██████████| 1/1 [00:00<00:00, 11.23it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 40.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.1863245964050293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35 Loss: 1.2433546781539917: 100%|██████████| 1/1 [00:00<00:00, 11.22it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 39.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.1841344833374023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36 Loss: 1.2564622163772583: 100%|██████████| 1/1 [00:00<00:00, 11.40it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 40.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.1819934844970703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37 Loss: 1.2660638093948364: 100%|██████████| 1/1 [00:00<00:00, 11.60it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 41.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.179776191711426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38 Loss: 1.2495698928833008: 100%|██████████| 1/1 [00:00<00:00, 11.50it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 37.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.1777288913726807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39 Loss: 1.2629313468933105: 100%|██████████| 1/1 [00:00<00:00, 11.57it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 40.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.1757473945617676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40 Loss: 1.2616034746170044: 100%|██████████| 1/1 [00:00<00:00, 11.13it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 40.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.1739845275878906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41 Loss: 1.2642381191253662: 100%|██████████| 1/1 [00:00<00:00, 11.13it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 39.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.172421455383301\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42 Loss: 1.2709161043167114: 100%|██████████| 1/1 [00:00<00:00, 10.56it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 38.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.1712453365325928\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43 Loss: 1.2335784435272217: 100%|██████████| 1/1 [00:00<00:00, 10.63it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 40.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.1701998710632324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44 Loss: 1.2604143619537354: 100%|██████████| 1/1 [00:00<00:00, 11.12it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 41.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.1693239212036133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45 Loss: 1.2297723293304443: 100%|██████████| 1/1 [00:00<00:00, 11.52it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 41.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.1686668395996094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46 Loss: 1.2537521123886108: 100%|██████████| 1/1 [00:00<00:00, 10.17it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 40.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.168198347091675\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47 Loss: 1.2352983951568604: 100%|██████████| 1/1 [00:00<00:00, 10.95it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 40.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.1678733825683594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48 Loss: 1.2543716430664062: 100%|██████████| 1/1 [00:00<00:00, 10.95it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 40.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.167703628540039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49 Loss: 1.2558895349502563: 100%|██████████| 1/1 [00:00<00:00, 11.18it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 41.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 3.167633056640625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50 Loss: 1.2419521808624268: 100%|██████████| 1/1 [00:00<00:00,  8.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000],\n",
            "        [ 0.2161,  0.9230,  5.4712],\n",
            "        [-0.0511,  1.3395,  5.7365],\n",
            "        [-9.1964, -2.0464,  2.1981],\n",
            "        [ 0.5511,  0.3403,  0.6324],\n",
            "        [ 0.2743,  2.1257,  1.9717],\n",
            "        [-0.2609, -0.5264, -0.0494],\n",
            "        [-9.5399, -2.5150,  2.6933],\n",
            "        [-9.4151, -2.2497,  2.4974],\n",
            "        [-9.5581, -2.2083,  2.0122],\n",
            "        [ 0.6165,  1.8323,  2.1593],\n",
            "        [-9.4999, -2.3287,  2.1330]], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "tensor([[  0.0000,   0.0000,   0.0000],\n",
            "        [ -1.0760,  -3.3350,  -3.4670],\n",
            "        [ -4.4130,  -6.6660,  -7.6410],\n",
            "        [ -7.0260,  -0.6310,  -9.7380],\n",
            "        [ -3.1140,   3.6950, -11.0880],\n",
            "        [ -2.5960,   8.3430, -14.4300],\n",
            "        [  1.2760,  10.8900, -15.9760],\n",
            "        [  7.1280,  12.5880, -19.2280],\n",
            "        [  8.5000,   9.1200, -14.2150],\n",
            "        [  4.5950,   6.0570,  -8.3870],\n",
            "        [  6.2800,   5.2940,  -2.2990],\n",
            "        [  6.1480,   2.7090,   3.4880]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 29.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.5535e-02, -1.2110e-02, -1.1210e-03],\n",
            "        [-6.5715e+00, -1.3966e+00,  1.3455e+00],\n",
            "        [-7.5796e-02,  9.1916e-01,  3.4378e+00],\n",
            "        [-9.3508e-02,  9.0464e-01,  3.4325e+00],\n",
            "        [ 6.9673e-01,  1.3625e+00,  1.3570e+00],\n",
            "        [-6.6602e+00, -1.3338e+00,  1.4451e+00],\n",
            "        [-1.7041e-01,  9.1772e-01,  3.3394e+00],\n",
            "        [ 6.4403e-01,  1.2785e+00,  1.4280e+00],\n",
            "        [-6.5306e+00, -1.2974e+00,  1.4343e+00],\n",
            "        [-1.3569e-01,  9.6197e-01,  3.3590e+00],\n",
            "        [ 4.1699e-02,  2.5668e-02,  2.4292e-02],\n",
            "        [-6.7032e+00, -1.2785e+00,  1.4380e+00],\n",
            "        [ 2.0491e-02,  6.6305e-02,  9.3706e-03],\n",
            "        [-3.0857e-01,  1.0066e+00,  3.3743e+00],\n",
            "        [ 3.3944e-03,  3.1775e-02,  2.3566e-02],\n",
            "        [-2.9409e-01,  9.6661e-01,  3.4242e+00],\n",
            "        [-2.6359e-01,  9.3664e-01,  3.4560e+00],\n",
            "        [-6.4876e+00, -1.3059e+00,  1.3606e+00],\n",
            "        [ 5.0856e-01,  1.3257e+00,  1.4153e+00],\n",
            "        [ 4.6202e-01,  1.3121e+00,  1.4745e+00],\n",
            "        [-1.5733e-01, -1.7128e-02,  6.3135e-02],\n",
            "        [-2.6859e-01,  9.1883e-01,  3.4704e+00],\n",
            "        [-6.5610e+00, -1.3732e+00,  1.3694e+00],\n",
            "        [-1.2048e-01,  9.3019e-01,  3.5312e+00],\n",
            "        [ 6.6291e-01,  1.1998e+00,  1.5103e+00],\n",
            "        [-9.7537e-02,  9.6429e-01,  3.5624e+00],\n",
            "        [-1.1948e-01,  9.8424e-01,  3.5463e+00],\n",
            "        [ 5.6798e-03, -4.7344e-02,  7.8602e-02],\n",
            "        [ 3.7987e-02,  1.5060e-03,  6.6104e-02],\n",
            "        [-1.2561e-01,  1.0236e+00,  3.5088e+00],\n",
            "        [ 9.2178e-02,  5.5150e-02,  8.2768e-02],\n",
            "        [-6.9028e-02,  1.0668e+00,  3.5082e+00],\n",
            "        [-6.4564e-02,  1.0795e+00,  3.4774e+00],\n",
            "        [-6.4492e+00, -1.3497e+00,  1.5305e+00],\n",
            "        [-1.6411e-01,  1.0431e+00,  3.4049e+00],\n",
            "        [-9.6734e-02,  7.1429e-03,  8.2208e-02],\n",
            "        [-8.4014e-02,  2.5289e-03,  7.3344e-02],\n",
            "        [-2.5522e-02,  1.3620e-02,  8.4648e-02],\n",
            "        [ 5.0620e-02,  3.2665e-02,  1.0845e-01],\n",
            "        [ 8.1558e-02,  3.9079e-02,  1.2822e-01],\n",
            "        [ 5.2536e-02,  2.0911e-02,  1.2760e-01],\n",
            "        [ 6.9954e-01,  1.2767e+00,  1.3888e+00],\n",
            "        [-1.6475e-01,  1.0570e+00,  3.4438e+00],\n",
            "        [-1.8014e-01,  1.0518e+00,  3.4803e+00]], device='cuda:0')\n",
            "tensor([[  0.0000,   0.0000,   0.0000],\n",
            "        [ -0.7070,  -5.9580,   0.2510],\n",
            "        [  1.8180, -11.1220,   2.6320],\n",
            "        [  4.2810, -13.9570,   5.7980],\n",
            "        [  8.0630, -12.8990,  10.3460],\n",
            "        [ 10.0480, -10.8730,  15.0080],\n",
            "        [ 14.9250, -12.3670,  21.8680],\n",
            "        [ 12.1820,  -4.7690,  20.6990],\n",
            "        [  8.0700,  -1.0030,  22.7590],\n",
            "        [  1.6860,  -1.4760,  26.4590],\n",
            "        [  0.2910,   3.8560,  22.3290],\n",
            "        [ -4.6130,   2.8170,  21.1950],\n",
            "        [ -8.9810,  -0.8830,  22.1090],\n",
            "        [-10.0990,  -5.0790,  25.4830],\n",
            "        [-10.1600,  -7.7970,  30.2510],\n",
            "        [ -9.6060,  -8.0900,  35.5960],\n",
            "        [ -7.6790,  -6.0100,  40.9080],\n",
            "        [ -7.2770,  -1.9900,  44.3900],\n",
            "        [ -8.9710,   2.7920,  45.7980],\n",
            "        [-12.9290,   6.6020,  45.0960],\n",
            "        [-20.1510,   8.3560,  46.8390],\n",
            "        [-17.4410,  12.4890,  44.2450],\n",
            "        [-21.8030,  19.2040,  41.7060],\n",
            "        [-19.4020,  19.0910,  36.8900],\n",
            "        [-19.7190,  21.6050,  31.9250],\n",
            "        [-10.8560,  24.1070,  30.1850],\n",
            "        [ -7.3700,  16.5360,  29.4260],\n",
            "        [ -5.5970,  13.7800,  33.5650],\n",
            "        [ -6.2600,   8.0210,  40.2460],\n",
            "        [ -3.0390,   3.7300,  40.8050],\n",
            "        [ -1.7020,  -0.9740,  39.5490],\n",
            "        [ -1.1780,  -6.4040,  36.9250],\n",
            "        [ -2.3910,  -9.0850,  32.1110],\n",
            "        [ -3.3590,  -8.3700,  26.7530],\n",
            "        [ -6.9840, -11.3870,  22.3600],\n",
            "        [ -7.1440,  -8.1660,  18.1260],\n",
            "        [ -4.9090,  -2.5630,  16.6210],\n",
            "        [ -0.4550,   0.9240,  16.4910],\n",
            "        [  5.5070,   2.2800,  16.8240],\n",
            "        [ 10.2920,   0.3150,  15.6880],\n",
            "        [ 12.8350,  -3.7520,  13.0430],\n",
            "        [ 13.6350,  -7.6020,   9.2060],\n",
            "        [ 12.1330, -11.0210,   4.0290],\n",
            "        [  9.4680, -11.2910,  -0.5540],\n",
            "        [  5.5820,  -8.8450,  -4.4550]], device='cuda:0')\n",
            "val loss: 3.1676154136657715\n",
            "save complete\n"
          ]
        }
      ],
      "source": [
        "version_name = 'test_set_1'\n",
        "\n",
        "best_val_loss=99999999999\n",
        "loss_df = pd.DataFrame(columns=['epoch', 'train_loss', 'val_loss'])\n",
        "\n",
        "# print(type(train_loader))\n",
        "\n",
        "# epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    tbar=tqdm(train_loader)\n",
        "    total_loss=0\n",
        "    train_loss=0\n",
        "\n",
        "    for idx, batch in enumerate(tbar):\n",
        "        #try:\n",
        "        sequence=batch['sequence'].cuda()\n",
        "        gt_xyz=batch['xyz'].squeeze().cuda()\n",
        "\n",
        "        mask=~torch.isnan(gt_xyz)\n",
        "\n",
        "        gt_xyz[torch.isnan(gt_xyz)]=0\n",
        "\n",
        "        pred_xyz = model(sequence).squeeze()\n",
        "        if epoch == epochs-1:\n",
        "              print(pred_xyz)\n",
        "              print(gt_xyz)\n",
        "\n",
        "        loss = dRMAE(pred_xyz,pred_xyz,gt_xyz,gt_xyz) + align_svd_mae(pred_xyz, gt_xyz)\n",
        "\n",
        "        if loss!=loss:\n",
        "            stop\n",
        "\n",
        "        try:\n",
        "          (loss/batch_size).backward()\n",
        "        except:\n",
        "          print(gt_xyz.shape)\n",
        "        # (loss/batch_size).backward()\n",
        "\n",
        "        if (idx+1)%batch_size==0 or idx+1 == len(tbar):\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            # scaler.scale(loss/batch_size).backward()\n",
        "            # scaler.unscale_(optimizer)\n",
        "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "            # scaler.step(optimizer)\n",
        "            # scaler.update()\n",
        "\n",
        "            if (epoch+1)>cos_epoch:\n",
        "                schedule.step()\n",
        "\n",
        "        total_loss += loss\n",
        "        train_loss = total_loss/(idx+1)\n",
        "\n",
        "        tbar.set_description(f\"Epoch {epoch + 1} Loss: {total_loss/(idx+1)}\")\n",
        "\n",
        "    ### Validation\n",
        "    tbar=tqdm(val_loader)\n",
        "\n",
        "    model.eval()\n",
        "    val_preds=[]\n",
        "    val_loss=0\n",
        "\n",
        "    for idx, batch in enumerate(tbar):\n",
        "        sequence=batch['sequence'].cuda()\n",
        "        gt_xyz=batch['xyz'].squeeze().cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred_xyz=model(sequence).squeeze()\n",
        "            if epoch == epochs-1:\n",
        "              print(pred_xyz)\n",
        "              print(gt_xyz)\n",
        "\n",
        "            loss = dRMAE(pred_xyz,pred_xyz,gt_xyz,gt_xyz) + align_svd_mae(pred_xyz, gt_xyz)\n",
        "\n",
        "        val_loss+=loss\n",
        "        val_preds.append([gt_xyz.cpu().numpy(),pred_xyz.cpu().numpy()])\n",
        "\n",
        "    val_loss=val_loss/len(tbar)\n",
        "    print(f\"val loss: {val_loss}\")\n",
        "\n",
        "    save_df = pd.DataFrame({'epoch': [epoch], 'train_loss': [train_loss.cpu().item()], 'val_loss': [val_loss.cpu().item()]})\n",
        "    loss_df = pd.concat([loss_df, save_df], ignore_index=True)\n",
        "\n",
        "    ## Check Best Loss .pt and Save\n",
        "    if val_loss<best_val_loss:\n",
        "        best_val_loss=val_loss\n",
        "        best_preds=val_preds\n",
        "        torch.save(model.state_dict(),f'/content/drive/MyDrive/RNA/WuSubSol/Save_Data/{version_name}_{epochs}_best.pt')\n",
        "\n",
        "    # 1.053595052265986 train loss after epoch 0\n",
        "torch.save(model.state_dict(),f'/content/drive/MyDrive/RNA/WuSubSol/Save_Data/{version_name}_{epochs}_last.pt')\n",
        "\n",
        "# Save Loss,\n",
        "\n",
        "loss_df.to_csv(f'/content/drive/MyDrive/RNA/WuSubSol/Save_Data/{version_name}_{epochs}_loss.csv', index=False)\n",
        "\n",
        "print('save complete')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Loss Graph"
      ],
      "metadata": {
        "id": "CFNZ0biDYkXV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyEnI8BYc2t9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "a7543154-4ef1-47bf-d9ae-aef161ec55c4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"dfb5bf56-aec7-4f73-8ca3-5603644944a2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"dfb5bf56-aec7-4f73-8ca3-5603644944a2\")) {                    Plotly.newPlot(                        \"dfb5bf56-aec7-4f73-8ca3-5603644944a2\",                        [{\"hovertemplate\":\"Epoch: %{x}\\u003cbr\\u003eTrain Loss: %{y:.4f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"line\":{\"color\":\"blue\"},\"mode\":\"lines+markers\",\"name\":\"Train Loss\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[1.8013463020324707,1.796398043632507,1.7817728519439695,1.753955364227295,1.7171471118927002,1.687034010887146,1.6689987182617188,1.626583218574524,1.5745735168457031,1.5800257921218872,1.531322002410889,1.5038418769836426,1.483441710472107,1.4734010696411133,1.4664329290390017,1.4174222946166992,1.4128222465515137,1.351910948753357,1.3972586393356323,1.3639214038848877,1.3444607257843018,1.3016530275344849,1.329411268234253,1.3014200925827026,1.3310610055923462,1.2982301712036133,1.3026700019836426,1.2981529235839844,1.2955529689788818,1.2875205278396606,1.2926946878433228,1.2682563066482544,1.241859793663025,1.2788927555084229,1.2433546781539917,1.2564622163772583,1.2660638093948364,1.2495698928833008,1.2629313468933103,1.2616034746170044,1.2642381191253662,1.2709161043167114,1.2335784435272217,1.2604143619537354,1.2297723293304443,1.2537521123886108,1.2352983951568604,1.2543716430664062,1.2558895349502563,1.2419521808624268],\"type\":\"scatter\"},{\"hovertemplate\":\"Epoch: %{x}\\u003cbr\\u003eVal Loss: %{y:.4f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"line\":{\"color\":\"red\"},\"mode\":\"lines+markers\",\"name\":\"Val Loss\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[3.43274474143982,3.4241013526916504,3.41202974319458,3.39818811416626,3.3834710121154785,3.368791341781616,3.3542368412017822,3.340333938598633,3.3278942108154297,3.3192789554595947,3.314365863800049,3.3089089393615723,3.304979085922241,3.300119400024414,3.2946505546569824,3.287490129470825,3.2788898944854736,3.268570899963379,3.257856845855713,3.24704909324646,3.238285541534424,3.2303311824798584,3.2234694957733154,3.2172346115112305,3.2124931812286377,3.208605527877808,3.20554780960083,3.202108860015869,3.199486494064331,3.1972150802612305,3.1948580741882324,3.1916985511779785,3.188926935195923,3.1863245964050293,3.1841344833374023,3.1819934844970703,3.179776191711426,3.1777288913726807,3.175747394561768,3.17398452758789,3.172421455383301,3.1712453365325928,3.1701998710632324,3.1693239212036133,3.1686668395996094,3.168198347091675,3.16787338256836,3.167703628540039,3.167633056640625,3.1676154136657715],\"type\":\"scatter\"},{\"marker\":{\"color\":\"green\",\"size\":10},\"mode\":\"markers+text\",\"name\":\"Min Val\",\"text\":[\"Min Val Loss: 3.1676 (Epoch 49)\"],\"textposition\":\"top center\",\"x\":[49],\"y\":[3.1676154136657715],\"type\":\"scatter\"},{\"marker\":{\"color\":\"green\",\"size\":10},\"mode\":\"markers+text\",\"name\":\"Min Train\",\"text\":[\"Min Train Loss: 1.2298 (Epoch 44)\"],\"textposition\":\"top center\",\"x\":[44],\"y\":[1.2297723293304443],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"legend\":{\"x\":0.01,\"y\":0.99},\"title\":{\"text\":\"Train & Validation Loss_lr=1e-05_nlayers=30\"},\"xaxis\":{\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}},\"hovermode\":\"x unified\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('dfb5bf56-aec7-4f73-8ca3-5603644944a2');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "epochs=config['epochs']\n",
        "# CSV 로드\n",
        "df = pd.read_csv(f'/content/drive/MyDrive/RNA/WuSubSol/Save_Data/{version_name}_{epochs}_loss.csv')\n",
        "# df = pd.read_csv\n",
        "epochs = df['epoch']\n",
        "train_loss = df['train_loss']\n",
        "val_loss = df['val_loss']\n",
        "\n",
        "# 최솟값 정보\n",
        "min_train_loss = train_loss.min()\n",
        "min_train_epoch = epochs[train_loss.idxmin()]\n",
        "min_val_loss = val_loss.min()\n",
        "min_val_epoch = epochs[val_loss.idxmin()]\n",
        "\n",
        "# 그래프 만들기\n",
        "fig = go.Figure()\n",
        "\n",
        "# Train Loss\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=epochs,\n",
        "    y=train_loss,\n",
        "    mode='lines+markers',\n",
        "    name='Train Loss',\n",
        "    line=dict(color='blue'),\n",
        "    hovertemplate='Epoch: %{x}<br>Train Loss: %{y:.4f}<extra></extra>'\n",
        "))\n",
        "\n",
        "# Val Loss\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=epochs,\n",
        "    y=val_loss,\n",
        "    mode='lines+markers',\n",
        "    name='Val Loss',\n",
        "    line=dict(color='red'),\n",
        "    hovertemplate='Epoch: %{x}<br>Val Loss: %{y:.4f}<extra></extra>'\n",
        "))\n",
        "\n",
        "# 최소값 표시\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=[min_val_epoch],\n",
        "    y=[min_val_loss],\n",
        "    mode='markers+text',\n",
        "    marker=dict(color='green', size=10),\n",
        "    text=[f\"Min Val Loss: {min_val_loss:.4f} (Epoch {min_val_epoch})\"],\n",
        "    textposition='top center',\n",
        "    name='Min Val',\n",
        "\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=[min_train_epoch],\n",
        "    y=[min_train_loss],\n",
        "    mode='markers+text',\n",
        "    marker=dict(color='green', size=10),\n",
        "    text=[f\"Min Train Loss: {min_train_loss:.4f} (Epoch {min_train_epoch})\"],\n",
        "    textposition='top center',\n",
        "    name='Min Train',\n",
        "))\n",
        "# 레이아웃 꾸미기\n",
        "fig.update_layout(\n",
        "    title=f\"Train & Validation Loss_lr={config['learning_rate']}_nlayers={config['n_layers']}\",\n",
        "    xaxis_title=\"Epoch\",\n",
        "    yaxis_title=\"Loss\",\n",
        "    hovermode='x unified',\n",
        "    template='plotly_white',\n",
        "    legend=dict(x=0.01, y=0.99)\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}